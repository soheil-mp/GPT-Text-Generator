{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mXEJ-Tp_GhXl"
      },
      "source": [
        "<br>\n",
        "\n",
        "<h1 style=\"text-align:center;\">Text generation with a miniature GPT</h1>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pJbw_wByGhXw"
      },
      "source": [
        "<br>\n",
        "\n",
        "### Introduction\n",
        "\n",
        "---\n",
        "\n",
        "Implementation of an autoregressive language model using the GPT model.\n",
        " \n",
        "We use the IMDB sentiment classification dataset for training generate new movie reviews for a given prompt.\n",
        "\n",
        "[GPT](https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035),\n",
        "[GPT-2](https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe),\n",
        "[GPT-3](https://arxiv.org/abs/2005.14165)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq62fG8qGhXy"
      },
      "source": [
        "<br>\n",
        "\n",
        "### INITIAL SETUP\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JdKgIbOSGhXz"
      },
      "outputs": [],
      "source": [
        "# Import the libraries\n",
        "import re, os, string, random\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2])>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.range(1, 3)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2J2Al3I-GhX2"
      },
      "source": [
        "<br>\n",
        "\n",
        "### TRANSFORMER BLOCK\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Causal attention mask function\n",
        "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
        "    \"\"\"\n",
        "    This function creates a causal attention mask for the transformer model.\n",
        "    More specifically, it will mask the upper half of the dot product matrix in \n",
        "    self attention (to prevent flow of information from future tokens to current).\n",
        "    ARGUMENTS\n",
        "    =================\n",
        "        - batch_size: batch size of the input\n",
        "        - n_dest: number of tokens in the destination sequence\n",
        "        - n_src: number of tokens in the source sequence\n",
        "        - dtype: data type of the mask\n",
        "        \n",
        "    RETURNS\n",
        "    =================\n",
        "        - out: causal attention mask\n",
        "    \"\"\"\n",
        "    \n",
        "    # Initialize the indices\n",
        "    i = tf.range(n_dest)[:, None]\n",
        "    j = tf.range(n_src)\n",
        "    \n",
        "    # Create the mask \n",
        "    m = i >= j - n_src + n_dest\n",
        "    \n",
        "    # Cast the mask\n",
        "    mask = tf.cast(m, dtype)\n",
        "    \n",
        "    # Expand the mask\n",
        "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
        "    \n",
        "    # This is the mask that we will use to mask the upper half of the dot product matrix\n",
        "    mult = tf.concat([tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0)\n",
        "    \n",
        "    # Tile the mask \n",
        "    out = tf.tile(mask, mult)\n",
        "    \n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lTozsQRkGhX3"
      },
      "outputs": [],
      "source": [
        "# Transformer block class\n",
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "\n",
        "    # Constructo function\n",
        "    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, rate: float=0.1):\n",
        "\n",
        "        # Inherite parent's constructor\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        # Self-attention layer\n",
        "        self.att = tf.keras.layers.MultiHeadAttention(num_heads, embed_dim)\n",
        "\n",
        "        # Feed forward network\n",
        "        self.ffn = tf.keras.Sequential(\n",
        "            [tf.keras.layers.Dense(ff_dim, activation=\"relu\"), tf.keras.layers.Dense(embed_dim),]\n",
        "        )\n",
        "\n",
        "        # Layer normalizations\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        # Dropouts\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    # Call function\n",
        "    def call(self, inputs: tf.Tensor, training: bool = False) -> tf.Tensor:\n",
        "        \n",
        "        # Initialize variables\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size = input_shape[0]\n",
        "        seq_len = input_shape[1]\n",
        "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
        "        \n",
        "        # First attention block\n",
        "        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
        "        attention_output = self.dropout1(attention_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attention_output)\n",
        "        \n",
        "        # Second attention block\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out = self.layernorm2(out1 + ffn_output)\n",
        "        \n",
        "        return out"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OL5kjkJ6GhX5"
      },
      "source": [
        "<br>\n",
        "\n",
        "### EMBEDDING LAYER\n",
        "\n",
        "---\n",
        "\n",
        "Create two seperate embedding layers: one for tokens and one for token index\n",
        "(positions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NqgmSDoSGhX6"
      },
      "outputs": [],
      "source": [
        "# Token and position embedding class\n",
        "class TokenAndPositionEmbedding(tf.keras.layers.Layer):\n",
        "    \n",
        "    # Initialize the constructor\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        \n",
        "        # Inherit the parent's constructor\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        \n",
        "        # Token embedding layer\n",
        "        self.token_emb = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        \n",
        "        # Position embedding layer\n",
        "        self.pos_emb = tf.keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    # Call function\n",
        "    def call(self, x):\n",
        "        \n",
        "        # Maximum sequence length\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        \n",
        "        # Initialize the positions\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        \n",
        "        # Feed the positions to the position embedding layer\n",
        "        positions = self.pos_emb(positions)\n",
        "        \n",
        "        # Feed the tokens to the token embedding layer\n",
        "        x = self.token_emb(x)\n",
        "        \n",
        "        # Add the token and position embeddings\n",
        "        out = x + positions\n",
        "        \n",
        "        return out"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mpcEw9Y5GhX8"
      },
      "source": [
        "<br>\n",
        "\n",
        "### GPT model\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialization\n",
        "vocab_size = 20000       # Vocabulary (only consider the top 20k words)\n",
        "maxlen = 80              # Max sequence size\n",
        "embed_dim = 256          # Embedding size for each token\n",
        "num_heads = 2            # Number of attention heads\n",
        "feed_forward_dim = 256   # Hidden layer size in feed forward network inside transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "X-A69BerGhX9"
      },
      "outputs": [],
      "source": [
        "# GPT model\n",
        "def create_model():\n",
        "    \n",
        "    # Input layer\n",
        "    inputs = tf.keras.layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
        "    \n",
        "    # Token and position embedding layer\n",
        "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "    x = embedding_layer(inputs)\n",
        "    \n",
        "    # Transformer block\n",
        "    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
        "    x = transformer_block(x)\n",
        "    \n",
        "    # Output layer\n",
        "    outputs = tf.keras.layers.Dense(vocab_size)(x)\n",
        "    \n",
        "    # Construct the model\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=[outputs, x])\n",
        "    \n",
        "    # Loss function\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    \n",
        "    # Compile the model\n",
        "    model.compile(\"adam\", loss=[loss_fn, None],)  # No loss and optimization based on word embeddings from transformer block\n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8nr2u-FtGhX_"
      },
      "source": [
        "<br>\n",
        "\n",
        "### DOWNLOAD AND PREPARE DATASET\n",
        "\n",
        "---\n",
        "\n",
        "Download the IMDB dataset and combine training and validation sets for a text\n",
        "generation task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pq7ZGuFxGhYA"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "  0 80.2M    0 32768    0     0  13415      0  1:44:31  0:00:02  1:44:29 13418\n",
            "  0 80.2M    0  352k    0     0   106k      0  0:12:47  0:00:03  0:12:44  107k\n",
            "  2 80.2M    2 2192k    0     0   513k      0  0:02:40  0:00:04  0:02:36  513k\n",
            "  7 80.2M    7 5872k    0     0  1101k      0  0:01:14  0:00:05  0:01:09 1169k\n",
            "  7 80.2M    7 5920k    0     0   934k      0  0:01:27  0:00:06  0:01:21 1175k\n",
            "  8 80.2M    8 7296k    0     0   993k      0  0:01:22  0:00:07  0:01:15 1480k\n",
            " 11 80.2M   11 9856k    0     0  1172k      0  0:01:10  0:00:08  0:01:02 1858k\n",
            " 12 80.2M   12 9936k    0     0  1055k      0  0:01:17  0:00:09  0:01:08 1505k\n",
            " 12 80.2M   12 9984k    0     0   957k      0  0:01:25  0:00:10  0:01:15  806k\n",
            " 12 80.2M   12  9.7M    0     0   877k      0  0:01:33  0:00:11  0:01:22  806k\n",
            " 12 80.2M   12  9.8M    0     0   812k      0  0:01:41  0:00:12  0:01:29  551k\n",
            " 12 80.2M   12  9.9M    0     0   759k      0  0:01:48  0:00:13  0:01:35 59566\n",
            " 12 80.2M   12  9.9M    0     0   709k      0  0:01:55  0:00:14  0:01:41 53044\n",
            " 12 80.2M   12  9.9M    0     0   665k      0  0:02:03  0:00:15  0:01:48 49779\n",
            " 12 80.2M   12 10.0M    0     0   627k      0  0:02:10  0:00:16  0:01:54 49779\n",
            " 12 80.2M   12 10.0M    0     0   591k      0  0:02:18  0:00:17  0:02:01 48588\n",
            " 14 80.2M   14 11.5M    0     0   647k      0  0:02:06  0:00:18  0:01:48  344k\n",
            " 21 80.2M   21 17.1M    0     0   908k      0  0:01:30  0:00:19  0:01:11 1489k\n",
            " 28 80.2M   28 22.4M    0     0  1134k      0  0:01:12  0:00:20  0:00:52 2598k\n",
            " 35 80.2M   35 28.1M    0     0  1354k      0  0:01:00  0:00:21  0:00:39 3770k\n",
            " 42 80.2M   42 34.0M    0     0  1563k      0  0:00:52  0:00:22  0:00:30 5070k\n",
            " 49 80.2M   49 40.0M    0     0  1760k      0  0:00:46  0:00:23  0:00:23 5804k\n",
            " 57 80.2M   57 46.1M    0     0  1945k      0  0:00:42  0:00:24  0:00:18 5943k\n",
            " 65 80.2M   65 52.2M    0     0  2115k      0  0:00:38  0:00:25  0:00:13 6097k\n",
            " 72 80.2M   72 58.3M    0     0  2273k      0  0:00:36  0:00:26  0:00:10 6191k\n",
            " 80 80.2M   80 64.4M    0     0  2418k      0  0:00:33  0:00:27  0:00:06 6243k\n",
            " 87 80.2M   87 70.5M    0     0  2555k      0  0:00:32  0:00:28  0:00:04 6278k\n",
            " 95 80.2M   95 76.3M    0     0  2670k      0  0:00:30  0:00:29  0:00:01 6192k\n",
            "100 80.2M  100 80.2M    0     0  2747k      0  0:00:29  0:00:29 --:--:-- 6208k\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset\n",
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Batch size\n",
        "batch_size = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50000 files\n"
          ]
        }
      ],
      "source": [
        "# Initialize a list for filesnames\n",
        "filenames = []\n",
        "\n",
        "# Loop over the directories\n",
        "for dir in [\"aclImdb/train/pos\", \"aclImdb/train/neg\", \"aclImdb/test/pos\", \"aclImdb/test/neg\"]:\n",
        "    \n",
        "    # Loop over the files inside each directory\n",
        "    for f in os.listdir(dir):\n",
        "        \n",
        "        # Append the filename to the list\n",
        "        filenames.append(os.path.join(dir, f))\n",
        "\n",
        "# Report\n",
        "print(f\"{len(filenames)} files\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Shuffle the filenames\n",
        "random.shuffle(filenames)\n",
        "\n",
        "# Load the dataset through tf.data\n",
        "text_ds = tf.data.TextLineDataset(filenames)\n",
        "\n",
        "# Shuffle the dataset\n",
        "text_ds = text_ds.shuffle(buffer_size=256)\n",
        "\n",
        "# Set the batch size\n",
        "text_ds = text_ds.batch(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "wTe5Z5_IGhYB"
      },
      "outputs": [],
      "source": [
        "# Function for custom standardization\n",
        "def custom_standardization(input_string):\n",
        "    \n",
        "    # Lowercase the text\n",
        "    lowercased = tf.strings.lower(input_string)\n",
        "    \n",
        "    # Remove html line-break tags\n",
        "    stripped_html = tf.strings.regex_replace(lowercased, \"<br />\", \" \")\n",
        "    \n",
        "    # Handle punctuation\n",
        "    out = tf.strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n",
        "    \n",
        "    return out\n",
        "\n",
        "\n",
        "# Text vectorization layer\n",
        "vectorize_layer = tf.keras.layers.TextVectorization(standardize=custom_standardization,\n",
        "                                                    max_tokens=vocab_size - 1,\n",
        "                                                    output_mode=\"int\",\n",
        "                                                    output_sequence_length=maxlen + 1,\n",
        ")\n",
        "\n",
        "# Adapt the vectorization layer to the text\n",
        "vectorize_layer.adapt(text_ds)\n",
        "\n",
        "# Vocabulary list\n",
        "vocab = vectorize_layer.get_vocabulary()  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function for preparing the inputs and labels\n",
        "def prepare_lm_inputs_labels(text):\n",
        "    \n",
        "    # Add the extra dimension to the text\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    \n",
        "    # Vectorize the text\n",
        "    tokenized_sentences = vectorize_layer(text)\n",
        "    \n",
        "    # Inputs (all words except the last)\n",
        "    x = tokenized_sentences[:, :-1]\n",
        "    \n",
        "    # Labels (shifted one position)\n",
        "    y = tokenized_sentences[:, 1:]\n",
        "    \n",
        "    return x, y\n",
        "\n",
        "# Map the function to the dataset\n",
        "text_ds = text_ds.map(prepare_lm_inputs_labels)\n",
        "\n",
        "# Prefetch the dataset\n",
        "text_ds = text_ds.prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hIfgyag5GhYD"
      },
      "source": [
        "<br>\n",
        "\n",
        "### CALLBACK FUNCTION\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "zcRD0dkkGhYE"
      },
      "outputs": [],
      "source": [
        "# Class for generating text\n",
        "class TextGenerator(tf.keras.callbacks.Callback):\n",
        "    \"\"\"A callback to generate text from a trained model.\n",
        "    1. Feed some starting prompt to the model\n",
        "    2. Predict probabilities for the next token\n",
        "    3. Sample the next token and add it to the next input\n",
        "\n",
        "    Arguments:\n",
        "        max_tokens: Integer, the number of tokens to be generated after prompt.\n",
        "        start_tokens: List of integers, the token indices for the starting prompt.\n",
        "        index_to_word: List of strings, obtained from the TextVectorization layer.\n",
        "        top_k: Integer, sample from the `top_k` token predictions.\n",
        "        print_every: Integer, print after this many epochs.\n",
        "    \"\"\"\n",
        "\n",
        "    # Constructor function\n",
        "    def __init__(self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1):\n",
        "        \n",
        "        # Initialization\n",
        "        self.max_tokens = max_tokens\n",
        "        self.start_tokens = start_tokens\n",
        "        self.index_to_word = index_to_word\n",
        "        self.print_every = print_every\n",
        "        self.k = top_k\n",
        "\n",
        "    # Function for sampling from the model\n",
        "    def sample_from(self, logits):\n",
        "        \n",
        "        # Finds values and indices of the k largest entries for the last dimension.\n",
        "        logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n",
        "        \n",
        "        # Covert indices to numpy array\n",
        "        indices = np.asarray(indices).astype(\"int32\")\n",
        "        \n",
        "        # Softmax to convert logits to probabilities\n",
        "        preds = tf.keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
        "        \n",
        "        # Convert to numpy array\n",
        "        preds = np.asarray(preds).astype(\"float32\")\n",
        "        \n",
        "        # Generates a random sample from a given 1-D array\n",
        "        out = np.random.choice(indices, p=preds)\n",
        "        \n",
        "        return out \n",
        "\n",
        "    # Function for converting indices to tokens\n",
        "    def detokenize(self, number):\n",
        "        \n",
        "        # Convert index to word\n",
        "        return self.index_to_word[number]\n",
        "\n",
        "    # Function for generating text\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        \n",
        "        # Initialize the start tokens \n",
        "        start_tokens = [_ for _ in self.start_tokens]\n",
        "        \n",
        "        # Every `print_every` epochs\n",
        "        if (epoch + 1) % self.print_every != 0:\n",
        "            \n",
        "            # Return\n",
        "            return\n",
        "        \n",
        "        # Initialize the number of tokens generated\n",
        "        num_tokens_generated = 0\n",
        "        \n",
        "        # Initialize the tokens generated\n",
        "        tokens_generated = []\n",
        "        \n",
        "        # Loop until the number of tokens generated is less than the maximum number of tokens\n",
        "        while num_tokens_generated <= self.max_tokens:\n",
        "            \n",
        "            # Pad length \n",
        "            pad_len = maxlen - len(start_tokens)\n",
        "            \n",
        "            # The index of the last token in the start_tokens\n",
        "            sample_index = len(start_tokens) - 1\n",
        "            \n",
        "            # If the pad length is less than 0\n",
        "            if pad_len < 0:\n",
        "                \n",
        "                # Inputs: Start tokens from 0 to maxlen\n",
        "                x = start_tokens[:maxlen]\n",
        "                \n",
        "                # Set the sample index to maxlen - 1\n",
        "                sample_index = maxlen - 1\n",
        "                \n",
        "            # If the pad length is greater than 0\n",
        "            elif pad_len > 0:\n",
        "                \n",
        "                # Inputs: Start tokens and pad with 0s\n",
        "                x = start_tokens + [0] * pad_len\n",
        "                \n",
        "            # If the pad length is 0\n",
        "            else:\n",
        "                \n",
        "                # Inputs: Start tokens\n",
        "                x = start_tokens\n",
        "                \n",
        "            # Convert to numpy array\n",
        "            x = np.array([x])\n",
        "            \n",
        "            # Predict\n",
        "            y, _ = self.model.predict(x)\n",
        "            \n",
        "            # Sample from the model\n",
        "            sample_token = self.sample_from(y[0][sample_index])\n",
        "            \n",
        "            # Append the sample token to the generated tokens\n",
        "            tokens_generated.append(sample_token)\n",
        "            \n",
        "            # Append the sample token to the start tokens\n",
        "            start_tokens.append(sample_token)\n",
        "            \n",
        "            # Increment the number of tokens generated\n",
        "            num_tokens_generated = len(tokens_generated)\n",
        "            \n",
        "        # Join the predicted tokens \n",
        "        txt = \" \".join([self.detokenize(_) for _ in self.start_tokens + tokens_generated])\n",
        "        \n",
        "        # Report\n",
        "        print(f\"GENERATED TEXT:\\n{txt}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize a word2index dictionary\n",
        "word_to_index = {}\n",
        "\n",
        "# Loop over the vocabulary\n",
        "for index, word in enumerate(vocab):\n",
        "    \n",
        "    # Add word and index to the dictionary\n",
        "    word_to_index[word] = index\n",
        "\n",
        "# Start prompt\n",
        "start_prompt = \"the police \"\n",
        "\n",
        "# Convert the start prompt to token indices\n",
        "start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n",
        "\n",
        "# Number of tokens to be generated\n",
        "num_tokens_generated = 50\n",
        "\n",
        "# Initialize the text generator\n",
        "text_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yMqEqaKDGhYG"
      },
      "source": [
        "<br>\n",
        "\n",
        "### TRAINING\n",
        "\n",
        "---\n",
        "\n",
        "Note: This code should preferably be run on GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 80)]              0         \n",
            "_________________________________________________________________\n",
            "token_and_position_embedding (None, 80, 256)           5140480   \n",
            "_________________________________________________________________\n",
            "transformer_block (Transform (None, 80, 256)           658688    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 80, 20000)         5140000   \n",
            "=================================================================\n",
            "Total params: 10,939,168\n",
            "Trainable params: 10,939,168\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Initialize the model\n",
        "model = create_model()\n",
        "\n",
        "# Model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "n3_5sUpJGhYH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 39s 93ms/step - loss: 5.5724 - dense_2_loss: 5.5724\n",
            "GENERATED TEXT:\n",
            "the police , is a little bit too well , and the only the worst movie i have ever seen . this movie is a very good movie that i had to find out of the [UNK] the same time and i was in my opinion , but when i am going to\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x1fab539b2e0>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the model\n",
        "model.fit(text_ds, verbose=1, epochs=1, callbacks=[text_gen_callback])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>\n",
        "\n",
        "### PREDICTION\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Class for generating text\n",
        "# class TextGenerator():\n",
        "\n",
        "#     # Constructor function\n",
        "#     def __init__(self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1):\n",
        "        \n",
        "#         # Initialization\n",
        "#         self.max_tokens = max_tokens\n",
        "#         self.start_tokens = start_tokens\n",
        "#         self.index_to_word = index_to_word\n",
        "#         self.print_every = print_every\n",
        "#         self.k = top_k\n",
        "\n",
        "#     # Function for sampling from the model\n",
        "#     def sample_from(self, logits):\n",
        "        \n",
        "#         # Finds values and indices of the k largest entries for the last dimension.\n",
        "#         logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n",
        "        \n",
        "#         # Covert indices to numpy array\n",
        "#         indices = np.asarray(indices).astype(\"int32\")\n",
        "        \n",
        "#         # Softmax to convert logits to probabilities\n",
        "#         preds = tf.keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
        "        \n",
        "#         # Convert to numpy array\n",
        "#         preds = np.asarray(preds).astype(\"float32\")\n",
        "        \n",
        "#         # Generates a random sample from a given 1-D array\n",
        "#         out = np.random.choice(indices, p=preds)\n",
        "        \n",
        "#         return out \n",
        "\n",
        "#     # Function for converting indices to tokens\n",
        "#     def detokenize(self, number):\n",
        "        \n",
        "#         # Convert index to word\n",
        "#         return self.index_to_word[number]\n",
        "\n",
        "#     # Function for generating text\n",
        "#     def on_epoch_end(self, epoch, logs=None):\n",
        "        \n",
        "#         # Initialize the start tokens \n",
        "#         start_tokens = [_ for _ in self.start_tokens]\n",
        "        \n",
        "#         # Every `print_every` epochs\n",
        "#         if (epoch + 1) % self.print_every != 0:\n",
        "            \n",
        "#             # Return\n",
        "#             return\n",
        "        \n",
        "#         # Initialize the number of tokens generated\n",
        "#         num_tokens_generated = 0\n",
        "        \n",
        "#         # Initialize the tokens generated\n",
        "#         tokens_generated = []\n",
        "        \n",
        "#         # Loop until the number of tokens generated is less than the maximum number of tokens\n",
        "#         while num_tokens_generated <= self.max_tokens:\n",
        "            \n",
        "#             # Pad length \n",
        "#             pad_len = maxlen - len(start_tokens)\n",
        "            \n",
        "#             # The index of the last token in the start_tokens\n",
        "#             sample_index = len(start_tokens) - 1\n",
        "            \n",
        "#             # If the pad length is less than 0\n",
        "#             if pad_len < 0:\n",
        "                \n",
        "#                 # Inputs: Start tokens from 0 to maxlen\n",
        "#                 x = start_tokens[:maxlen]\n",
        "                \n",
        "#                 # Set the sample index to maxlen - 1\n",
        "#                 sample_index = maxlen - 1\n",
        "                \n",
        "#             # If the pad length is greater than 0\n",
        "#             elif pad_len > 0:\n",
        "                \n",
        "#                 # Inputs: Start tokens and pad with 0s\n",
        "#                 x = start_tokens + [0] * pad_len\n",
        "                \n",
        "#             # If the pad length is 0\n",
        "#             else:\n",
        "                \n",
        "#                 # Inputs: Start tokens\n",
        "#                 x = start_tokens\n",
        "                \n",
        "#             # Convert to numpy array\n",
        "#             x = np.array([x])\n",
        "            \n",
        "#             # Predict\n",
        "#             y, _ = self.model.predict(x)\n",
        "            \n",
        "#             # Sample from the model\n",
        "#             sample_token = self.sample_from(y[0][sample_index])\n",
        "            \n",
        "#             # Append the sample token to the generated tokens\n",
        "#             tokens_generated.append(sample_token)\n",
        "            \n",
        "#             # Append the sample token to the start tokens\n",
        "#             start_tokens.append(sample_token)\n",
        "            \n",
        "#             # Increment the number of tokens generated\n",
        "#             num_tokens_generated = len(tokens_generated)\n",
        "            \n",
        "#         # Join the predicted tokens \n",
        "#         txt = \" \".join([self.detokenize(_) for _ in self.start_tokens + tokens_generated])\n",
        "        \n",
        "#         # Report\n",
        "#         print(f\"GENERATED TEXT:\\n{txt}\\n\")\n",
        "\n",
        "# # Initialize a word2index dictionary\n",
        "# word_to_index = {}\n",
        "\n",
        "# # Loop over the vocabulary\n",
        "# for index, word in enumerate(vocab):\n",
        "    \n",
        "#     # Add word and index to the dictionary\n",
        "#     word_to_index[word] = index\n",
        "\n",
        "# # Start prompt\n",
        "# start_prompt = \"the wizard \"\n",
        "\n",
        "# # Convert the start prompt to token indices\n",
        "# start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n",
        "\n",
        "# # Number of tokens to be generated\n",
        "# num_tokens_generated = 50\n",
        "\n",
        "# # Initialize the text generator\n",
        "# text_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>\n",
        "\n",
        "### EVALUATION\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "text_generation_with_miniature_gpt",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "c992f0b96e0bf1b7bf4ed1da6ffb7bb064b2bb3e7201b712e1347652797d5077"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
