{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mXEJ-Tp_GhXl"
      },
      "source": [
        "<br>\n",
        "\n",
        "<h1 style=\"text-align:center;\">Text generation with a miniature GPT</h1>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pJbw_wByGhXw"
      },
      "source": [
        "<br>\n",
        "\n",
        "### Introduction\n",
        "\n",
        "---\n",
        "\n",
        "Implementation of an autoregressive language model using the GPT model.\n",
        " \n",
        "We use the IMDB sentiment classification dataset for training generate new movie reviews for a given prompt.\n",
        "\n",
        "[GPT](https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035),\n",
        "[GPT-2](https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe),\n",
        "[GPT-3](https://arxiv.org/abs/2005.14165)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq62fG8qGhXy"
      },
      "source": [
        "<br>\n",
        "\n",
        "### INITIAL SETUP\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JdKgIbOSGhXz"
      },
      "outputs": [],
      "source": [
        "# Import the libraries\n",
        "import re, os, string, random\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2])>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.range(1, 3)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2J2Al3I-GhX2"
      },
      "source": [
        "<br>\n",
        "\n",
        "### TRANSFORMER BLOCK\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Causal attention mask function\n",
        "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
        "    \"\"\"\n",
        "    This function creates a causal attention mask for the transformer model.\n",
        "    More specifically, it will mask the upper half of the dot product matrix in \n",
        "    self attention (to prevent flow of information from future tokens to current).\n",
        "    ARGUMENTS\n",
        "    =================\n",
        "        - batch_size: batch size of the input\n",
        "        - n_dest: number of tokens in the destination sequence\n",
        "        - n_src: number of tokens in the source sequence\n",
        "        - dtype: data type of the mask\n",
        "        \n",
        "    RETURNS\n",
        "    =================\n",
        "        - out: causal attention mask\n",
        "    \"\"\"\n",
        "    \n",
        "    # Initialize the indices\n",
        "    i = tf.range(n_dest)[:, None]\n",
        "    j = tf.range(n_src)\n",
        "    \n",
        "    # Create the mask \n",
        "    m = i >= j - n_src + n_dest\n",
        "    \n",
        "    # Cast the mask\n",
        "    mask = tf.cast(m, dtype)\n",
        "    \n",
        "    # Expand the mask\n",
        "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
        "    \n",
        "    # This is the mask that we will use to mask the upper half of the dot product matrix\n",
        "    mult = tf.concat([tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0)\n",
        "    \n",
        "    # Tile the mask \n",
        "    out = tf.tile(mask, mult)\n",
        "    \n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lTozsQRkGhX3"
      },
      "outputs": [],
      "source": [
        "# Transformer block class\n",
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "\n",
        "    # Constructo function\n",
        "    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, rate: float=0.1):\n",
        "\n",
        "        # Inherite parent's constructor\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        # Self-attention layer\n",
        "        self.att = tf.keras.layers.MultiHeadAttention(num_heads, embed_dim)\n",
        "\n",
        "        # Feed forward network\n",
        "        self.ffn = tf.keras.Sequential(\n",
        "            [tf.keras.layers.Dense(ff_dim, activation=\"relu\"), tf.keras.layers.Dense(embed_dim),]\n",
        "        )\n",
        "\n",
        "        # Layer normalizations\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        # Dropouts\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    # Call function\n",
        "    def call(self, inputs: tf.Tensor, training: bool = False) -> tf.Tensor:\n",
        "        \n",
        "        # Initialize variables\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size = input_shape[0]\n",
        "        seq_len = input_shape[1]\n",
        "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
        "        \n",
        "        # First attention block\n",
        "        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
        "        attention_output = self.dropout1(attention_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attention_output)\n",
        "        \n",
        "        # Second attention block\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out = self.layernorm2(out1 + ffn_output)\n",
        "        \n",
        "        return out"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OL5kjkJ6GhX5"
      },
      "source": [
        "<br>\n",
        "\n",
        "### EMBEDDING LAYER\n",
        "\n",
        "---\n",
        "\n",
        "Create two seperate embedding layers: one for tokens and one for token index\n",
        "(positions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NqgmSDoSGhX6"
      },
      "outputs": [],
      "source": [
        "# Token and position embedding class\n",
        "class TokenAndPositionEmbedding(tf.keras.layers.Layer):\n",
        "    \n",
        "    # Initialize the constructor\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        \n",
        "        # Inherit the parent's constructor\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        \n",
        "        # Token embedding layer\n",
        "        self.token_emb = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        \n",
        "        # Position embedding layer\n",
        "        self.pos_emb = tf.keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    # Call function\n",
        "    def call(self, x):\n",
        "        \n",
        "        # Maximum sequence length\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        \n",
        "        # Initialize the positions\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        \n",
        "        # Feed the positions to the position embedding layer\n",
        "        positions = self.pos_emb(positions)\n",
        "        \n",
        "        # Feed the tokens to the token embedding layer\n",
        "        x = self.token_emb(x)\n",
        "        \n",
        "        # Add the token and position embeddings\n",
        "        out = x + positions\n",
        "        \n",
        "        return out"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mpcEw9Y5GhX8"
      },
      "source": [
        "<br>\n",
        "\n",
        "### GPT model\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialization\n",
        "vocab_size = 20000       # Vocabulary (only consider the top 20k words)\n",
        "maxlen = 80              # Max sequence size\n",
        "embed_dim = 256          # Embedding size for each token\n",
        "num_heads = 2            # Number of attention heads\n",
        "feed_forward_dim = 256   # Hidden layer size in feed forward network inside transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "X-A69BerGhX9"
      },
      "outputs": [],
      "source": [
        "# GPT model\n",
        "def create_model():\n",
        "    \n",
        "    # Input layer\n",
        "    inputs = tf.keras.layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
        "    \n",
        "    # Token and position embedding layer\n",
        "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "    x = embedding_layer(inputs)\n",
        "    \n",
        "    # Transformer block\n",
        "    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
        "    x = transformer_block(x)\n",
        "    \n",
        "    # Output layer\n",
        "    outputs = tf.keras.layers.Dense(vocab_size)(x)\n",
        "    \n",
        "    # Construct the model\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=[outputs, x])\n",
        "    \n",
        "    # Loss function\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    \n",
        "    # Compile the model\n",
        "    model.compile(\"adam\", loss=[loss_fn, None],)  # No loss and optimization based on word embeddings from transformer block\n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8nr2u-FtGhX_"
      },
      "source": [
        "<br>\n",
        "\n",
        "### DOWNLOAD AND PREPARE DATASET\n",
        "\n",
        "---\n",
        "\n",
        "Download the IMDB dataset and combine training and validation sets for a text\n",
        "generation task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pq7ZGuFxGhYA"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "  0 80.2M    0 32768    0     0  14546      0  1:36:23  0:00:02  1:36:21 14557\n",
            "  0 80.2M    0  432k    0     0   131k      0  0:10:25  0:00:03  0:10:22  131k\n",
            "  4 80.2M    4 3440k    0     0   819k      0  0:01:40  0:00:04  0:01:36  819k\n",
            " 15 80.2M   15 12.7M    0     0  2515k      0  0:00:32  0:00:05  0:00:27 2626k\n",
            " 28 80.2M   28 22.9M    0     0  3782k      0  0:00:21  0:00:06  0:00:15 4692k\n",
            " 42 80.2M   42 34.3M    0     0  4892k      0  0:00:16  0:00:07  0:00:09 7113k\n",
            " 56 80.2M   56 45.4M    0     0  5672k      0  0:00:14  0:00:08  0:00:06 9380k\n",
            " 72 80.2M   72 57.8M    0     0  6440k      0  0:00:12  0:00:09  0:00:03 10.8M\n",
            " 84 80.2M   84 67.8M    0     0  6764k      0  0:00:12  0:00:10  0:00:02 10.8M\n",
            " 96 80.2M   96 77.6M    0     0  7103k      0  0:00:11  0:00:11 --:--:-- 10.9M\n",
            "100 80.2M  100 80.2M    0     0  7111k      0  0:00:11  0:00:11 --:--:-- 10.5M\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset\n",
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Batch size\n",
        "batch_size = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15889 files\n"
          ]
        }
      ],
      "source": [
        "# Initialize a list for filesnames\n",
        "filenames = []\n",
        "\n",
        "# Loop over the directories\n",
        "for dir in [\"aclImdb/train/pos\", \"aclImdb/train/neg\", \"aclImdb/test/pos\", \"aclImdb/test/neg\"]:\n",
        "    \n",
        "    # Loop over the files inside each directory\n",
        "    for f in os.listdir(dir):\n",
        "        \n",
        "        # Append the filename to the list\n",
        "        filenames.append(os.path.join(dir, f))\n",
        "\n",
        "# Report\n",
        "print(f\"{len(filenames)} files\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Shuffle the filenames\n",
        "random.shuffle(filenames)\n",
        "\n",
        "# Load the dataset through tf.data\n",
        "text_ds = tf.data.TextLineDataset(filenames)\n",
        "\n",
        "# Shuffle the dataset\n",
        "text_ds = text_ds.shuffle(buffer_size=256)\n",
        "\n",
        "# Set the batch size\n",
        "text_ds = text_ds.batch(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wTe5Z5_IGhYB"
      },
      "outputs": [],
      "source": [
        "# Function for custom standardization\n",
        "def custom_standardization(input_string):\n",
        "    \n",
        "    # Lowercase the text\n",
        "    lowercased = tf.strings.lower(input_string)\n",
        "    \n",
        "    # Remove html line-break tags\n",
        "    stripped_html = tf.strings.regex_replace(lowercased, \"<br />\", \" \")\n",
        "    \n",
        "    # Handle punctuation\n",
        "    out = tf.strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n",
        "    \n",
        "    return out\n",
        "\n",
        "\n",
        "# Text vectorization layer\n",
        "vectorize_layer = tf.keras.layers.TextVectorization(standardize=custom_standardization,\n",
        "                                                    max_tokens=vocab_size - 1,\n",
        "                                                    output_mode=\"int\",\n",
        "                                                    output_sequence_length=maxlen + 1,\n",
        ")\n",
        "\n",
        "# Adapt the vectorization layer to the text\n",
        "vectorize_layer.adapt(text_ds)\n",
        "\n",
        "# Vocabulary list\n",
        "vocab = vectorize_layer.get_vocabulary()  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "in user code:\n\n\n    TypeError: outer_factory.<locals>.inner_factory.<locals>.tf__prepare_lm_inputs_labels() takes 1 positional argument but 2 were given\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[17], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[39mreturn\u001b[39;00m x, y\n\u001b[0;32m     18\u001b[0m \u001b[39m# Map the function to the dataset\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m text_ds \u001b[39m=\u001b[39m text_ds\u001b[39m.\u001b[39mmap(prepare_lm_inputs_labels)\n\u001b[0;32m     21\u001b[0m \u001b[39m# Prefetch the dataset\u001b[39;00m\n\u001b[0;32m     22\u001b[0m text_ds \u001b[39m=\u001b[39m text_ds\u001b[39m.\u001b[39mprefetch(tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mAUTOTUNE)\n",
            "File \u001b[1;32mc:\\Users\\Soheil\\anaconda3\\envs\\tensorflow\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:2240\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[1;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[0;32m   2236\u001b[0m \u001b[39m# Loaded lazily due to a circular dependency (dataset_ops -> map_op ->\u001b[39;00m\n\u001b[0;32m   2237\u001b[0m \u001b[39m# dataset_ops).\u001b[39;00m\n\u001b[0;32m   2238\u001b[0m \u001b[39m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[0;32m   2239\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m map_op\n\u001b[1;32m-> 2240\u001b[0m \u001b[39mreturn\u001b[39;00m map_op\u001b[39m.\u001b[39m_map_v2(\n\u001b[0;32m   2241\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   2242\u001b[0m     map_func,\n\u001b[0;32m   2243\u001b[0m     num_parallel_calls\u001b[39m=\u001b[39mnum_parallel_calls,\n\u001b[0;32m   2244\u001b[0m     deterministic\u001b[39m=\u001b[39mdeterministic,\n\u001b[0;32m   2245\u001b[0m     name\u001b[39m=\u001b[39mname)\n",
            "File \u001b[1;32mc:\\Users\\Soheil\\anaconda3\\envs\\tensorflow\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\map_op.py:37\u001b[0m, in \u001b[0;36m_map_v2\u001b[1;34m(input_dataset, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[0;32m     34\u001b[0m   \u001b[39mif\u001b[39;00m deterministic \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m debug_mode\u001b[39m.\u001b[39mDEBUG_MODE:\n\u001b[0;32m     35\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mThe `deterministic` argument has no effect unless the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     36\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39m`num_parallel_calls` argument is specified.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m   \u001b[39mreturn\u001b[39;00m _MapDataset(\n\u001b[0;32m     38\u001b[0m       input_dataset, map_func, preserve_cardinality\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, name\u001b[39m=\u001b[39mname)\n\u001b[0;32m     39\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m   \u001b[39mreturn\u001b[39;00m _ParallelMapDataset(\n\u001b[0;32m     41\u001b[0m       input_dataset,\n\u001b[0;32m     42\u001b[0m       map_func,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m       preserve_cardinality\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     46\u001b[0m       name\u001b[39m=\u001b[39mname)\n",
            "File \u001b[1;32mc:\\Users\\Soheil\\anaconda3\\envs\\tensorflow\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\map_op.py:107\u001b[0m, in \u001b[0;36m_MapDataset.__init__\u001b[1;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_inter_op_parallelism \u001b[39m=\u001b[39m use_inter_op_parallelism\n\u001b[0;32m    106\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preserve_cardinality \u001b[39m=\u001b[39m preserve_cardinality\n\u001b[1;32m--> 107\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func \u001b[39m=\u001b[39m structured_function\u001b[39m.\u001b[39mStructuredFunctionWrapper(\n\u001b[0;32m    108\u001b[0m     map_func,\n\u001b[0;32m    109\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transformation_name(),\n\u001b[0;32m    110\u001b[0m     dataset\u001b[39m=\u001b[39minput_dataset,\n\u001b[0;32m    111\u001b[0m     use_legacy_function\u001b[39m=\u001b[39muse_legacy_function)\n\u001b[0;32m    112\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name \u001b[39m=\u001b[39m name\n\u001b[0;32m    113\u001b[0m variant_tensor \u001b[39m=\u001b[39m gen_dataset_ops\u001b[39m.\u001b[39mmap_dataset(\n\u001b[0;32m    114\u001b[0m     input_dataset\u001b[39m.\u001b[39m_variant_tensor,  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func\u001b[39m.\u001b[39mfunction\u001b[39m.\u001b[39mcaptured_inputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    118\u001b[0m     preserve_cardinality\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preserve_cardinality,\n\u001b[0;32m    119\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_common_args)\n",
            "File \u001b[1;32mc:\\Users\\Soheil\\anaconda3\\envs\\tensorflow\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:261\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m    254\u001b[0m       warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    255\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    256\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    257\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    258\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    259\u001b[0m     fn_factory \u001b[39m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[1;32m--> 261\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function \u001b[39m=\u001b[39m fn_factory()\n\u001b[0;32m    262\u001b[0m \u001b[39m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[0;32m    263\u001b[0m add_to_graph \u001b[39m&\u001b[39m\u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly()\n",
            "File \u001b[1;32mc:\\Users\\Soheil\\anaconda3\\envs\\tensorflow\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:232\u001b[0m, in \u001b[0;36mTracingCompiler.get_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_concrete_function\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    224\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Returns a `ConcreteFunction` specialized to inputs and execution context.\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \n\u001b[0;32m    226\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[39m      `tf.Tensor` or `tf.TensorSpec`.\u001b[39;00m\n\u001b[0;32m    231\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m   concrete_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_concrete_function_garbage_collected(\n\u001b[0;32m    233\u001b[0m       \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    234\u001b[0m   concrete_function\u001b[39m.\u001b[39m_garbage_collector\u001b[39m.\u001b[39mrelease()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    235\u001b[0m   \u001b[39mreturn\u001b[39;00m concrete_function\n",
            "File \u001b[1;32mc:\\Users\\Soheil\\anaconda3\\envs\\tensorflow\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:202\u001b[0m, in \u001b[0;36mTracingCompiler._get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_spec\u001b[39m.\u001b[39mmake_canonicalized_monomorphic_type(args, kwargs)\n\u001b[0;32m    201\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m--> 202\u001b[0m   concrete_function, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_concrete_function(args, kwargs)\n\u001b[0;32m    203\u001b[0m   seen_names \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[0;32m    204\u001b[0m   concrete_function\u001b[39m.\u001b[39m_arg_keywords \u001b[39m=\u001b[39m []  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Soheil\\anaconda3\\envs\\tensorflow\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:166\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_concrete_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m   args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_signature\n\u001b[0;32m    164\u001b[0m   kwargs \u001b[39m=\u001b[39m {}\n\u001b[1;32m--> 166\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n",
            "File \u001b[1;32mc:\\Users\\Soheil\\anaconda3\\envs\\tensorflow\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:396\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m    393\u001b[0m   args \u001b[39m=\u001b[39m placeholder_bound_args\u001b[39m.\u001b[39margs\n\u001b[0;32m    394\u001b[0m kwargs \u001b[39m=\u001b[39m placeholder_bound_args\u001b[39m.\u001b[39mkwargs\n\u001b[1;32m--> 396\u001b[0m concrete_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_concrete_function(\n\u001b[0;32m    397\u001b[0m     args, kwargs, func_graph)\n\u001b[0;32m    399\u001b[0m \u001b[39m# TODO(b/263520817): Remove access to private attribute.\u001b[39;00m\n\u001b[0;32m    400\u001b[0m graph_capture_container \u001b[39m=\u001b[39m concrete_function\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39m_function_captures  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Soheil\\anaconda3\\envs\\tensorflow\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:300\u001b[0m, in \u001b[0;36mTracingCompiler._create_concrete_function\u001b[1;34m(self, args, kwargs, func_graph)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    297\u001b[0m   arg_names \u001b[39m=\u001b[39m base_arg_names\n\u001b[0;32m    299\u001b[0m concrete_function \u001b[39m=\u001b[39m monomorphic_function\u001b[39m.\u001b[39mConcreteFunction(\n\u001b[1;32m--> 300\u001b[0m     func_graph_module\u001b[39m.\u001b[39mfunc_graph_from_py_func(\n\u001b[0;32m    301\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name,\n\u001b[0;32m    302\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_python_function,\n\u001b[0;32m    303\u001b[0m         args,\n\u001b[0;32m    304\u001b[0m         kwargs,\n\u001b[0;32m    305\u001b[0m         \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    306\u001b[0m         func_graph\u001b[39m=\u001b[39mfunc_graph,\n\u001b[0;32m    307\u001b[0m         autograph\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_autograph,\n\u001b[0;32m    308\u001b[0m         autograph_options\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_autograph_options,\n\u001b[0;32m    309\u001b[0m         arg_names\u001b[39m=\u001b[39marg_names,\n\u001b[0;32m    310\u001b[0m         capture_by_value\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_capture_by_value,\n\u001b[0;32m    311\u001b[0m         create_placeholders\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m),\n\u001b[0;32m    312\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_attributes,\n\u001b[0;32m    313\u001b[0m     spec\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_spec,\n\u001b[0;32m    314\u001b[0m     \u001b[39m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[0;32m    315\u001b[0m     \u001b[39m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[0;32m    316\u001b[0m     \u001b[39m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[0;32m    317\u001b[0m     \u001b[39m# ConcreteFunction.\u001b[39;00m\n\u001b[0;32m    318\u001b[0m     shared_func_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    319\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\n",
            "File \u001b[1;32mc:\\Users\\Soheil\\anaconda3\\envs\\tensorflow\\Lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1214\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders, acd_record_initial_resource_uses)\u001b[0m\n\u001b[0;32m   1211\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1212\u001b[0m   _, original_func \u001b[39m=\u001b[39m tf_decorator\u001b[39m.\u001b[39munwrap(python_func)\n\u001b[1;32m-> 1214\u001b[0m func_outputs \u001b[39m=\u001b[39m python_func(\u001b[39m*\u001b[39mfunc_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfunc_kwargs)\n\u001b[0;32m   1216\u001b[0m \u001b[39m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[0;32m   1217\u001b[0m \u001b[39m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[0;32m   1218\u001b[0m func_outputs \u001b[39m=\u001b[39m variable_utils\u001b[39m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
            "File \u001b[1;32mc:\\Users\\Soheil\\anaconda3\\envs\\tensorflow\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:238\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.trace_tf_function.<locals>.wrapped_fn\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[39m@eager_function\u001b[39m\u001b[39m.\u001b[39mdefun_with_attributes(\n\u001b[0;32m    233\u001b[0m     input_signature\u001b[39m=\u001b[39mstructure\u001b[39m.\u001b[39mget_flat_tensor_specs(\n\u001b[0;32m    234\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_structure),\n\u001b[0;32m    235\u001b[0m     autograph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    236\u001b[0m     attributes\u001b[39m=\u001b[39mdefun_kwargs)\n\u001b[0;32m    237\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_fn\u001b[39m(\u001b[39m*\u001b[39margs):  \u001b[39m# pylint: disable=missing-docstring\u001b[39;00m\n\u001b[1;32m--> 238\u001b[0m   ret \u001b[39m=\u001b[39m wrapper_helper(\u001b[39m*\u001b[39margs)\n\u001b[0;32m    239\u001b[0m   ret \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39mto_tensor_list(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_structure, ret)\n\u001b[0;32m    240\u001b[0m   \u001b[39mreturn\u001b[39;00m [ops\u001b[39m.\u001b[39mconvert_to_tensor(t) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m ret]\n",
            "File \u001b[1;32mc:\\Users\\Soheil\\anaconda3\\envs\\tensorflow\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:169\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.wrapper_helper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _should_unpack(nested_args):\n\u001b[0;32m    168\u001b[0m   nested_args \u001b[39m=\u001b[39m (nested_args,)\n\u001b[1;32m--> 169\u001b[0m ret \u001b[39m=\u001b[39m autograph\u001b[39m.\u001b[39mtf_convert(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_func, ag_ctx)(\u001b[39m*\u001b[39mnested_args)\n\u001b[0;32m    170\u001b[0m ret \u001b[39m=\u001b[39m variable_utils\u001b[39m.\u001b[39mconvert_variables_to_tensors(ret)\n\u001b[0;32m    171\u001b[0m \u001b[39mif\u001b[39;00m _should_pack(ret):\n",
            "File \u001b[1;32mc:\\Users\\Soheil\\anaconda3\\envs\\tensorflow\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:692\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    691\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 692\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[0;32m    693\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    694\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Soheil\\anaconda3\\envs\\tensorflow\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:689\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    688\u001b[0m   \u001b[39mwith\u001b[39;00m conversion_ctx:\n\u001b[1;32m--> 689\u001b[0m     \u001b[39mreturn\u001b[39;00m converted_call(f, args, kwargs, options\u001b[39m=\u001b[39moptions)\n\u001b[0;32m    690\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    691\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m'\u001b[39m):\n",
            "File \u001b[1;32mc:\\Users\\Soheil\\anaconda3\\envs\\tensorflow\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    438\u001b[0m   \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 439\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    440\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    441\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args)\n",
            "\u001b[1;31mTypeError\u001b[0m: in user code:\n\n\n    TypeError: outer_factory.<locals>.inner_factory.<locals>.tf__prepare_lm_inputs_labels() takes 1 positional argument but 2 were given\n"
          ]
        }
      ],
      "source": [
        "# Function for preparing the inputs and labels\n",
        "def prepare_lm_inputs_labels(text):\n",
        "    \n",
        "    # Add the extra dimension to the text\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    \n",
        "    # Vectorize the text\n",
        "    tokenized_sentences = vectorize_layer(text)\n",
        "    \n",
        "    # Inputs (all words except the last)\n",
        "    x = tokenized_sentences[:, :-1]\n",
        "    \n",
        "    # Labels (shifted one position)\n",
        "    y = tokenized_sentences[:, 1:]\n",
        "    \n",
        "    return x, y\n",
        "\n",
        "# Map the function to the dataset\n",
        "text_ds = text_ds.map(prepare_lm_inputs_labels)\n",
        "\n",
        "# Prefetch the dataset\n",
        "text_ds = text_ds.prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hIfgyag5GhYD"
      },
      "source": [
        "<br>\n",
        "\n",
        "### CALLBACK FUNCTION\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "zcRD0dkkGhYE"
      },
      "outputs": [],
      "source": [
        "# Class for generating text\n",
        "class TextGenerator(tf.keras.callbacks.Callback):\n",
        "    \"\"\"A callback to generate text from a trained model.\n",
        "    1. Feed some starting prompt to the model\n",
        "    2. Predict probabilities for the next token\n",
        "    3. Sample the next token and add it to the next input\n",
        "\n",
        "    Arguments:\n",
        "        max_tokens: Integer, the number of tokens to be generated after prompt.\n",
        "        start_tokens: List of integers, the token indices for the starting prompt.\n",
        "        index_to_word: List of strings, obtained from the TextVectorization layer.\n",
        "        top_k: Integer, sample from the `top_k` token predictions.\n",
        "        print_every: Integer, print after this many epochs.\n",
        "    \"\"\"\n",
        "\n",
        "    # Constructor function\n",
        "    def __init__(self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1):\n",
        "        \n",
        "        # Initialization\n",
        "        self.max_tokens = max_tokens\n",
        "        self.start_tokens = start_tokens\n",
        "        self.index_to_word = index_to_word\n",
        "        self.print_every = print_every\n",
        "        self.k = top_k\n",
        "\n",
        "    # Function for sampling from the model\n",
        "    def sample_from(self, logits):\n",
        "        \n",
        "        # Finds values and indices of the k largest entries for the last dimension.\n",
        "        logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n",
        "        \n",
        "        # Covert indices to numpy array\n",
        "        indices = np.asarray(indices).astype(\"int32\")\n",
        "        \n",
        "        # Softmax to convert logits to probabilities\n",
        "        preds = tf.keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
        "        \n",
        "        # Convert to numpy array\n",
        "        preds = np.asarray(preds).astype(\"float32\")\n",
        "        \n",
        "        # Generates a random sample from a given 1-D array\n",
        "        out = np.random.choice(indices, p=preds)\n",
        "        \n",
        "        return out \n",
        "\n",
        "    # Function for converting indices to tokens\n",
        "    def detokenize(self, number):\n",
        "        \n",
        "        # Convert index to word\n",
        "        return self.index_to_word[number]\n",
        "\n",
        "    # Function for generating text\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        \n",
        "        # Initialize the start tokens \n",
        "        start_tokens = [_ for _ in self.start_tokens]\n",
        "        \n",
        "        # Every `print_every` epochs\n",
        "        if (epoch + 1) % self.print_every != 0:\n",
        "            \n",
        "            # Return\n",
        "            return\n",
        "        \n",
        "        # Initialize the number of tokens generated\n",
        "        num_tokens_generated = 0\n",
        "        \n",
        "        # Initialize the tokens generated\n",
        "        tokens_generated = []\n",
        "        \n",
        "        # Loop until the number of tokens generated is less than the maximum number of tokens\n",
        "        while num_tokens_generated <= self.max_tokens:\n",
        "            \n",
        "            # Pad length \n",
        "            pad_len = maxlen - len(start_tokens)\n",
        "            \n",
        "            # The index of the last token in the start_tokens\n",
        "            sample_index = len(start_tokens) - 1\n",
        "            \n",
        "            # If the pad length is less than 0\n",
        "            if pad_len < 0:\n",
        "                \n",
        "                # Inputs: Start tokens from 0 to maxlen\n",
        "                x = start_tokens[:maxlen]\n",
        "                \n",
        "                # Set the sample index to maxlen - 1\n",
        "                sample_index = maxlen - 1\n",
        "                \n",
        "            # If the pad length is greater than 0\n",
        "            elif pad_len > 0:\n",
        "                \n",
        "                # Inputs: Start tokens and pad with 0s\n",
        "                x = start_tokens + [0] * pad_len\n",
        "                \n",
        "            # If the pad length is 0\n",
        "            else:\n",
        "                \n",
        "                # Inputs: Start tokens\n",
        "                x = start_tokens\n",
        "                \n",
        "            # Convert to numpy array\n",
        "            x = np.array([x])\n",
        "            \n",
        "            # Predict\n",
        "            y, _ = self.model.predict(x)\n",
        "            \n",
        "            # Sample from the model\n",
        "            sample_token = self.sample_from(y[0][sample_index])\n",
        "            \n",
        "            # Append the sample token to the generated tokens\n",
        "            tokens_generated.append(sample_token)\n",
        "            \n",
        "            # Append the sample token to the start tokens\n",
        "            start_tokens.append(sample_token)\n",
        "            \n",
        "            # Increment the number of tokens generated\n",
        "            num_tokens_generated = len(tokens_generated)\n",
        "            \n",
        "        # Join the predicted tokens \n",
        "        txt = \" \".join([self.detokenize(_) for _ in self.start_tokens + tokens_generated])\n",
        "        \n",
        "        # Report\n",
        "        print(f\"GENERATED TEXT:\\n{txt}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize a word2index dictionary\n",
        "word_to_index = {}\n",
        "\n",
        "# Loop over the vocabulary\n",
        "for index, word in enumerate(vocab):\n",
        "    \n",
        "    # Add word and index to the dictionary\n",
        "    word_to_index[word] = index\n",
        "\n",
        "# Start prompt\n",
        "start_prompt = \"the police \"\n",
        "\n",
        "# Convert the start prompt to token indices\n",
        "start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n",
        "\n",
        "# Number of tokens to be generated\n",
        "num_tokens_generated = 50\n",
        "\n",
        "# Initialize the text generator\n",
        "text_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yMqEqaKDGhYG"
      },
      "source": [
        "<br>\n",
        "\n",
        "### TRAINING\n",
        "\n",
        "---\n",
        "\n",
        "Note: This code should preferably be run on GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 80)]              0         \n",
            "_________________________________________________________________\n",
            "token_and_position_embedding (None, 80, 256)           5140480   \n",
            "_________________________________________________________________\n",
            "transformer_block (Transform (None, 80, 256)           658688    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 80, 20000)         5140000   \n",
            "=================================================================\n",
            "Total params: 10,939,168\n",
            "Trainable params: 10,939,168\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Initialize the model\n",
        "model = create_model()\n",
        "\n",
        "# Model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "n3_5sUpJGhYH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 39s 93ms/step - loss: 5.5724 - dense_2_loss: 5.5724\n",
            "GENERATED TEXT:\n",
            "the police , is a little bit too well , and the only the worst movie i have ever seen . this movie is a very good movie that i had to find out of the [UNK] the same time and i was in my opinion , but when i am going to\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x1fab539b2e0>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the model\n",
        "model.fit(text_ds, verbose=1, epochs=1, callbacks=[text_gen_callback])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>\n",
        "\n",
        "### PREDICTION\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Class for generating text\n",
        "# class TextGenerator():\n",
        "\n",
        "#     # Constructor function\n",
        "#     def __init__(self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1):\n",
        "        \n",
        "#         # Initialization\n",
        "#         self.max_tokens = max_tokens\n",
        "#         self.start_tokens = start_tokens\n",
        "#         self.index_to_word = index_to_word\n",
        "#         self.print_every = print_every\n",
        "#         self.k = top_k\n",
        "\n",
        "#     # Function for sampling from the model\n",
        "#     def sample_from(self, logits):\n",
        "        \n",
        "#         # Finds values and indices of the k largest entries for the last dimension.\n",
        "#         logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n",
        "        \n",
        "#         # Covert indices to numpy array\n",
        "#         indices = np.asarray(indices).astype(\"int32\")\n",
        "        \n",
        "#         # Softmax to convert logits to probabilities\n",
        "#         preds = tf.keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
        "        \n",
        "#         # Convert to numpy array\n",
        "#         preds = np.asarray(preds).astype(\"float32\")\n",
        "        \n",
        "#         # Generates a random sample from a given 1-D array\n",
        "#         out = np.random.choice(indices, p=preds)\n",
        "        \n",
        "#         return out \n",
        "\n",
        "#     # Function for converting indices to tokens\n",
        "#     def detokenize(self, number):\n",
        "        \n",
        "#         # Convert index to word\n",
        "#         return self.index_to_word[number]\n",
        "\n",
        "#     # Function for generating text\n",
        "#     def on_epoch_end(self, epoch, logs=None):\n",
        "        \n",
        "#         # Initialize the start tokens \n",
        "#         start_tokens = [_ for _ in self.start_tokens]\n",
        "        \n",
        "#         # Every `print_every` epochs\n",
        "#         if (epoch + 1) % self.print_every != 0:\n",
        "            \n",
        "#             # Return\n",
        "#             return\n",
        "        \n",
        "#         # Initialize the number of tokens generated\n",
        "#         num_tokens_generated = 0\n",
        "        \n",
        "#         # Initialize the tokens generated\n",
        "#         tokens_generated = []\n",
        "        \n",
        "#         # Loop until the number of tokens generated is less than the maximum number of tokens\n",
        "#         while num_tokens_generated <= self.max_tokens:\n",
        "            \n",
        "#             # Pad length \n",
        "#             pad_len = maxlen - len(start_tokens)\n",
        "            \n",
        "#             # The index of the last token in the start_tokens\n",
        "#             sample_index = len(start_tokens) - 1\n",
        "            \n",
        "#             # If the pad length is less than 0\n",
        "#             if pad_len < 0:\n",
        "                \n",
        "#                 # Inputs: Start tokens from 0 to maxlen\n",
        "#                 x = start_tokens[:maxlen]\n",
        "                \n",
        "#                 # Set the sample index to maxlen - 1\n",
        "#                 sample_index = maxlen - 1\n",
        "                \n",
        "#             # If the pad length is greater than 0\n",
        "#             elif pad_len > 0:\n",
        "                \n",
        "#                 # Inputs: Start tokens and pad with 0s\n",
        "#                 x = start_tokens + [0] * pad_len\n",
        "                \n",
        "#             # If the pad length is 0\n",
        "#             else:\n",
        "                \n",
        "#                 # Inputs: Start tokens\n",
        "#                 x = start_tokens\n",
        "                \n",
        "#             # Convert to numpy array\n",
        "#             x = np.array([x])\n",
        "            \n",
        "#             # Predict\n",
        "#             y, _ = self.model.predict(x)\n",
        "            \n",
        "#             # Sample from the model\n",
        "#             sample_token = self.sample_from(y[0][sample_index])\n",
        "            \n",
        "#             # Append the sample token to the generated tokens\n",
        "#             tokens_generated.append(sample_token)\n",
        "            \n",
        "#             # Append the sample token to the start tokens\n",
        "#             start_tokens.append(sample_token)\n",
        "            \n",
        "#             # Increment the number of tokens generated\n",
        "#             num_tokens_generated = len(tokens_generated)\n",
        "            \n",
        "#         # Join the predicted tokens \n",
        "#         txt = \" \".join([self.detokenize(_) for _ in self.start_tokens + tokens_generated])\n",
        "        \n",
        "#         # Report\n",
        "#         print(f\"GENERATED TEXT:\\n{txt}\\n\")\n",
        "\n",
        "# # Initialize a word2index dictionary\n",
        "# word_to_index = {}\n",
        "\n",
        "# # Loop over the vocabulary\n",
        "# for index, word in enumerate(vocab):\n",
        "    \n",
        "#     # Add word and index to the dictionary\n",
        "#     word_to_index[word] = index\n",
        "\n",
        "# # Start prompt\n",
        "# start_prompt = \"the wizard \"\n",
        "\n",
        "# # Convert the start prompt to token indices\n",
        "# start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n",
        "\n",
        "# # Number of tokens to be generated\n",
        "# num_tokens_generated = 50\n",
        "\n",
        "# # Initialize the text generator\n",
        "# text_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>\n",
        "\n",
        "### EVALUATION\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Exception encountered when calling layer \"transformer_block_1\" (type TransformerBlock).\n\nin user code:\n\n    File \"C:\\Users\\Soheil\\AppData\\Local\\Temp\\ipykernel_15100\\3558770377.py\", line 29, in call  *\n        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool, batch_size=batch_size)\n\n    TypeError: outer_factory.<locals>.inner_factory.<locals>.tf__causal_attention_mask() got multiple values for argument 'batch_size'\n\n\nCall arguments received by layer \"transformer_block_1\" (type TransformerBlock):\n   inputs=tf.Tensor(shape=(None, 80, 256), dtype=float32)\n   training=None",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 72\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n\u001b[0;32m     71\u001b[0m \u001b[39m# Initialize the model\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m model \u001b[39m=\u001b[39m create_transformer_model()\n\u001b[0;32m     73\u001b[0m \u001b[39mprint\u001b[39m(model\u001b[39m.\u001b[39msummary())\n",
            "Cell \u001b[1;32mIn[3], line 64\u001b[0m, in \u001b[0;36mcreate_transformer_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     62\u001b[0m x \u001b[39m=\u001b[39m embedding_layer(inputs)\n\u001b[0;32m     63\u001b[0m transformer_block \u001b[39m=\u001b[39m TransformerBlock(embed_dim, num_heads, feed_forward_dim, dropout_rate)\n\u001b[1;32m---> 64\u001b[0m x \u001b[39m=\u001b[39m transformer_block(x)\n\u001b[0;32m     65\u001b[0m outputs \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDense(vocab_size)(x)\n\u001b[0;32m     66\u001b[0m loss_fn \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "File \u001b[1;32mc:\\Users\\Soheil\\anaconda3\\envs\\tensorflow\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileul26ntqd.py:13\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, inputs, training)\u001b[0m\n\u001b[0;32m     11\u001b[0m batch_size \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(input_shape)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     12\u001b[0m seq_len \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(input_shape)[\u001b[39m1\u001b[39m]\n\u001b[1;32m---> 13\u001b[0m causal_mask \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(causal_attention_mask), (ag__\u001b[39m.\u001b[39mld(batch_size), ag__\u001b[39m.\u001b[39mld(seq_len), ag__\u001b[39m.\u001b[39mld(seq_len), ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mbool), \u001b[39mdict\u001b[39m(batch_size\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(batch_size)), fscope)\n\u001b[0;32m     14\u001b[0m attention_output \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mmulti_head_attention, (ag__\u001b[39m.\u001b[39mld(inputs), ag__\u001b[39m.\u001b[39mld(inputs)), \u001b[39mdict\u001b[39m(attention_mask\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(causal_mask)), fscope)\n\u001b[0;32m     15\u001b[0m attention_output \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mdropout1, (ag__\u001b[39m.\u001b[39mld(attention_output),), \u001b[39mdict\u001b[39m(training\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(training)), fscope)\n",
            "\u001b[1;31mTypeError\u001b[0m: Exception encountered when calling layer \"transformer_block_1\" (type TransformerBlock).\n\nin user code:\n\n    File \"C:\\Users\\Soheil\\AppData\\Local\\Temp\\ipykernel_15100\\3558770377.py\", line 29, in call  *\n        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool, batch_size=batch_size)\n\n    TypeError: outer_factory.<locals>.inner_factory.<locals>.tf__causal_attention_mask() got multiple values for argument 'batch_size'\n\n\nCall arguments received by layer \"transformer_block_1\" (type TransformerBlock):\n   inputs=tf.Tensor(shape=(None, 80, 256), dtype=float32)\n   training=None"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def causal_attention_mask(batch_size, dest_seq_len, src_seq_len, dtype):\n",
        "    i, j = tf.range(dest_seq_len)[:, None], tf.range(src_seq_len)\n",
        "    mask = i >= (j - src_seq_len + dest_seq_len)\n",
        "    mask = tf.cast(mask, dtype)\n",
        "    mask = tf.reshape(mask, [1, dest_seq_len, src_seq_len])\n",
        "    matrix_mult = tf.concat([tf.expand_dims(batch_size), tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "    out = tf.tile(mask, matrix_mult)\n",
        "    return out\n",
        "\n",
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.multi_head_attention = tf.keras.layers.MultiHeadAttention(num_heads, embed_dim)\n",
        "        self.feed_forward_network = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(feed_forward_dim, activation=\"relu\"),\n",
        "            tf.keras.layers.Dense(embed_dim)\n",
        "        ])\n",
        "        self.layer_norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layer_norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size = input_shape[0]\n",
        "        seq_len = input_shape[1]\n",
        "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool, batch_size=batch_size)\n",
        "        attention_output = self.multi_head_attention(inputs, inputs, attention_mask=causal_mask)\n",
        "        attention_output = self.dropout1(attention_output, training=training)\n",
        "        out1 = self.layer_norm1(inputs + attention_output)\n",
        "        ffn_output = self.feed_forward_network(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out = self.layer_norm2(out1 + ffn_output)\n",
        "        return out\n",
        "\n",
        "class TokenAndPositionEmbedding(tf.keras.layers.Layer):\n",
        "    def __init__(self, max_seq_len, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.position_embedding = tf.keras.layers.Embedding(input_dim=max_seq_len, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        max_seq_len = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=max_seq_len, delta=1)\n",
        "        positions = self.position_embedding(positions)\n",
        "        x = self.token_embedding(x)\n",
        "        out = x + positions\n",
        "        return out\n",
        "\n",
        "vocab_size = 20000\n",
        "max_seq_len = 80\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "feed_forward_dim = 256\n",
        "dropout_rate = 0.1\n",
        "\n",
        "def create_transformer_model():\n",
        "    inputs = tf.keras.layers.Input(shape=(max_seq_len, ), dtype=tf.int32)\n",
        "    embedding_layer = TokenAndPositionEmbedding(max_seq_len, vocab_size, embed_dim)\n",
        "    x = embedding_layer(inputs)\n",
        "    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim, dropout_rate)\n",
        "    x = transformer_block(x)\n",
        "    outputs = tf.keras.layers.Dense(vocab_size)(x)\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=[outputs, x])\n",
        "    model.compile(\"adam\", loss=[loss_fn, None])\n",
        "    return model\n",
        "\n",
        "# Initialize the model\n",
        "model = create_transformer_model()\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "text_generation_with_miniature_gpt",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "c992f0b96e0bf1b7bf4ed1da6ffb7bb064b2bb3e7201b712e1347652797d5077"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
