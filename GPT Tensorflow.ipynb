{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mXEJ-Tp_GhXl"
      },
      "source": [
        "<br>\n",
        "\n",
        "<h1 style=\"text-align:center;\">Text generation with a miniature GPT</h1>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pJbw_wByGhXw"
      },
      "source": [
        "<br>\n",
        "\n",
        "### Introduction\n",
        "\n",
        "---\n",
        "\n",
        "Implementation of an autoregressive language model using the GPT model.\n",
        " \n",
        "We use the IMDB sentiment classification dataset for training generate new movie reviews for a given prompt.\n",
        "\n",
        "[GPT](https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035),\n",
        "[GPT-2](https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe),\n",
        "[GPT-3](https://arxiv.org/abs/2005.14165)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq62fG8qGhXy"
      },
      "source": [
        "<br>\n",
        "\n",
        "### INITIAL SETUP\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JdKgIbOSGhXz"
      },
      "outputs": [],
      "source": [
        "# Import the libraries\n",
        "import re, os, string, random\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2J2Al3I-GhX2"
      },
      "source": [
        "<br>\n",
        "\n",
        "### TRANSFORMER BLOCK\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Causal attention mask function\n",
        "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
        "    \"\"\"\n",
        "    This function creates a causal attention mask for the transformer model.\n",
        "    More specifically, it will mask the upper half of the dot product matrix in \n",
        "    self attention (to prevent flow of information from future tokens to current).\n",
        "    ARGUMENTS\n",
        "    =================\n",
        "        - batch_size: batch size of the input\n",
        "        - n_dest: number of tokens in the destination sequence\n",
        "        - n_src: number of tokens in the source sequence\n",
        "        - dtype: data type of the mask\n",
        "        \n",
        "    RETURNS\n",
        "    =================\n",
        "        - out: causal attention mask\n",
        "    \"\"\"\n",
        "    \n",
        "    # Initialize the indices\n",
        "    i = tf.range(n_dest)[:, None]\n",
        "    j = tf.range(n_src)\n",
        "    \n",
        "    # Create the mask \n",
        "    m = i >= j - n_src + n_dest\n",
        "    \n",
        "    # Cast the mask\n",
        "    mask = tf.cast(m, dtype)\n",
        "    \n",
        "    # Expand the mask\n",
        "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
        "    \n",
        "    # This is the mask that we will use to mask the upper half of the dot product matrix\n",
        "    mult = tf.concat([tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0)\n",
        "    \n",
        "    # Tile the mask \n",
        "    out = tf.tile(mask, mult)\n",
        "    \n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lTozsQRkGhX3"
      },
      "outputs": [],
      "source": [
        "# Transformer block class\n",
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    \n",
        "    # Initialize the constructor\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        \n",
        "        # Inherit the parent's constructor\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        \n",
        "        # Multi-head attention layer\n",
        "        self.att = tf.keras.layers.MultiHeadAttention(num_heads, embed_dim)\n",
        "        \n",
        "        # Feed forward network\n",
        "        self.ffn = tf.keras.Sequential(\n",
        "            [tf.keras.layers.Dense(ff_dim, activation=\"relu\"), tf.keras.layers.Dense(embed_dim),]\n",
        "        )\n",
        "        \n",
        "        # Layer normalization\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        # Dropout regularization\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    # Call function\n",
        "    def call(self, inputs):\n",
        "        \n",
        "        # Input shape\n",
        "        input_shape = tf.shape(inputs)\n",
        "        \n",
        "        # Batch size\n",
        "        batch_size = input_shape[0]\n",
        "        \n",
        "        # Sequence length\n",
        "        seq_len = input_shape[1]\n",
        "        \n",
        "        # Causal mask\n",
        "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
        "        \n",
        "        # Multi-head attention with causal mask\n",
        "        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
        "        \n",
        "        # Dropout regularization\n",
        "        attention_output = self.dropout1(attention_output)\n",
        "        \n",
        "        # Add and normalize layers\n",
        "        out1 = self.layernorm1(inputs + attention_output)\n",
        "        \n",
        "        # Feed forward network\n",
        "        ffn_output = self.ffn(out1)\n",
        "        \n",
        "        # Dropout regularization\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        \n",
        "        # Add and normalize layers\n",
        "        out = self.layernorm2(out1 + ffn_output)\n",
        "        \n",
        "        return out\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OL5kjkJ6GhX5"
      },
      "source": [
        "<br>\n",
        "\n",
        "### EMBEDDING LAYER\n",
        "\n",
        "---\n",
        "\n",
        "Create two seperate embedding layers: one for tokens and one for token index\n",
        "(positions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NqgmSDoSGhX6"
      },
      "outputs": [],
      "source": [
        "# Token and position embedding class\n",
        "class TokenAndPositionEmbedding(tf.keras.layers.Layer):\n",
        "    \n",
        "    # Initialize the constructor\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        \n",
        "        # Inherit the parent's constructor\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        \n",
        "        # Token embedding layer\n",
        "        self.token_emb = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        \n",
        "        # Position embedding layer\n",
        "        self.pos_emb = tf.keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    # Call function\n",
        "    def call(self, x):\n",
        "        \n",
        "        # Maximum sequence length\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        \n",
        "        # Initialize the positions\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        \n",
        "        # Feed the positions to the position embedding layer\n",
        "        positions = self.pos_emb(positions)\n",
        "        \n",
        "        # Feed the tokens to the token embedding layer\n",
        "        x = self.token_emb(x)\n",
        "        \n",
        "        # Add the token and position embeddings\n",
        "        out = x + positions\n",
        "        \n",
        "        return out"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mpcEw9Y5GhX8"
      },
      "source": [
        "<br>\n",
        "\n",
        "### GPT model\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialization\n",
        "vocab_size = 20000       # Vocabulary (only consider the top 20k words)\n",
        "maxlen = 80              # Max sequence size\n",
        "embed_dim = 256          # Embedding size for each token\n",
        "num_heads = 2            # Number of attention heads\n",
        "feed_forward_dim = 256   # Hidden layer size in feed forward network inside transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "X-A69BerGhX9"
      },
      "outputs": [],
      "source": [
        "# GPT model\n",
        "def create_model():\n",
        "    \n",
        "    # Input layer\n",
        "    inputs = tf.keras.layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
        "    \n",
        "    # Token and position embedding layer\n",
        "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "    x = embedding_layer(inputs)\n",
        "    \n",
        "    # Transformer block\n",
        "    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
        "    x = transformer_block(x)\n",
        "    \n",
        "    # Output layer\n",
        "    outputs = tf.keras.layers.Dense(vocab_size)(x)\n",
        "    \n",
        "    # Construct the model\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=[outputs, x])\n",
        "    \n",
        "    # Loss function\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    \n",
        "    # Compile the model\n",
        "    model.compile(\"adam\", loss=[loss_fn, None],)  # No loss and optimization based on word embeddings from transformer block\n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8nr2u-FtGhX_"
      },
      "source": [
        "<br>\n",
        "\n",
        "### DOWNLOAD AND PREPARE DATASET\n",
        "\n",
        "---\n",
        "\n",
        "Download the IMDB dataset and combine training and validation sets for a text\n",
        "generation task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pq7ZGuFxGhYA"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0 80.2M    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "  0 80.2M    0 98304    0     0  53556      0  0:26:10  0:00:01  0:26:09 53571\n",
            "  0 80.2M    0  816k    0     0   287k      0  0:04:46  0:00:02  0:04:44  287k\n",
            "  6 80.2M    6 4992k    0     0  1156k      0  0:01:11  0:00:04  0:01:07 1156k\n",
            "  6 80.2M    6 5008k    0     0  1016k      0  0:01:20  0:00:04  0:01:16 1016k\n",
            "  8 80.2M    8 7200k    0     0  1210k      0  0:01:07  0:00:05  0:01:02 1464k\n",
            " 10 80.2M   10 8720k    0     0  1272k      0  0:01:04  0:00:06  0:00:58 1718k\n",
            " 17 80.2M   17 14.3M    0     0  1867k      0  0:00:43  0:00:07  0:00:36 2762k\n",
            " 23 80.2M   23 19.1M    0     0  2218k      0  0:00:37  0:00:08  0:00:29 3233k\n",
            " 28 80.2M   28 23.1M    0     0  2407k      0  0:00:34  0:00:09  0:00:25 3805k\n",
            " 34 80.2M   34 27.5M    0     0  2600k      0  0:00:31  0:00:10  0:00:21 4289k\n",
            " 40 80.2M   40 32.1M    0     0  2780k      0  0:00:29  0:00:11  0:00:18 4853k\n",
            " 46 80.2M   46 37.0M    0     0  2952k      0  0:00:27  0:00:12  0:00:15 4660k\n",
            " 52 80.2M   52 42.0M    0     0  3105k      0  0:00:26  0:00:13  0:00:13 4665k\n",
            " 58 80.2M   58 47.0M    0     0  3246k      0  0:00:25  0:00:14  0:00:11 4895k\n",
            " 64 80.2M   64 52.0M    0     0  3364k      0  0:00:24  0:00:15  0:00:09 5014k\n",
            " 71 80.2M   71 56.9M    0     0  3466k      0  0:00:23  0:00:16  0:00:07 5090k\n",
            " 77 80.2M   77 61.8M    0     0  3550k      0  0:00:23  0:00:17  0:00:06 5088k\n",
            " 83 80.2M   83 66.8M    0     0  3634k      0  0:00:22  0:00:18  0:00:04 5108k\n",
            " 89 80.2M   89 71.8M    0     0  3707k      0  0:00:22  0:00:19  0:00:03 5073k\n",
            " 95 80.2M   95 76.8M    0     0  3778k      0  0:00:21  0:00:20  0:00:01 5096k\n",
            "100 80.2M  100 80.2M    0     0  3827k      0  0:00:21  0:00:21 --:--:-- 5139k\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset\n",
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Batch size\n",
        "batch_size = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50000 files\n"
          ]
        }
      ],
      "source": [
        "# Initialize a list for filesnames\n",
        "filenames = []\n",
        "\n",
        "# Loop over the directories\n",
        "for dir in [\"aclImdb/train/pos\", \"aclImdb/train/neg\", \"aclImdb/test/pos\", \"aclImdb/test/neg\"]:\n",
        "    \n",
        "    # Loop over the files inside each directory\n",
        "    for f in os.listdir(dir):\n",
        "        \n",
        "        # Append the filename to the list\n",
        "        filenames.append(os.path.join(dir, f))\n",
        "\n",
        "# Report\n",
        "print(f\"{len(filenames)} files\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Shuffle the filenames\n",
        "random.shuffle(filenames)\n",
        "\n",
        "# Load the dataset through tf.data\n",
        "text_ds = tf.data.TextLineDataset(filenames)\n",
        "\n",
        "# Shuffle the dataset\n",
        "text_ds = text_ds.shuffle(buffer_size=256)\n",
        "\n",
        "# Set the batch size\n",
        "text_ds = text_ds.batch(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wTe5Z5_IGhYB"
      },
      "outputs": [],
      "source": [
        "# Function for custom standardization\n",
        "def custom_standardization(input_string):\n",
        "    \n",
        "    # Lowercase the text\n",
        "    lowercased = tf.strings.lower(input_string)\n",
        "    \n",
        "    # Remove html line-break tags\n",
        "    stripped_html = tf.strings.regex_replace(lowercased, \"<br />\", \" \")\n",
        "    \n",
        "    # Handle punctuation\n",
        "    out = tf.strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n",
        "    \n",
        "    return out\n",
        "\n",
        "\n",
        "# Text vectorization layer\n",
        "vectorize_layer = tf.keras.layers.TextVectorization(standardize=custom_standardization,\n",
        "                                                    max_tokens=vocab_size - 1,\n",
        "                                                    output_mode=\"int\",\n",
        "                                                    output_sequence_length=maxlen + 1,\n",
        ")\n",
        "\n",
        "# Adapt the vectorization layer to the text\n",
        "vectorize_layer.adapt(text_ds)\n",
        "\n",
        "# Vocabulary list\n",
        "vocab = vectorize_layer.get_vocabulary()  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function for preparing the inputs and labels\n",
        "def prepare_lm_inputs_labels(text):\n",
        "    \n",
        "    # Add the extra dimension to the text\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    \n",
        "    # Vectorize the text\n",
        "    tokenized_sentences = vectorize_layer(text)\n",
        "    \n",
        "    # Inputs (all words except the last)\n",
        "    x = tokenized_sentences[:, :-1]\n",
        "    \n",
        "    # Labels (shifted one position)\n",
        "    y = tokenized_sentences[:, 1:]\n",
        "    \n",
        "    return x, y\n",
        "\n",
        "# Map the function to the dataset\n",
        "text_ds = text_ds.map(prepare_lm_inputs_labels)\n",
        "\n",
        "# Prefetch the dataset\n",
        "text_ds = text_ds.prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hIfgyag5GhYD"
      },
      "source": [
        "<br>\n",
        "\n",
        "### CALLBACK FUNCTION\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zcRD0dkkGhYE"
      },
      "outputs": [],
      "source": [
        "# Class for generating text\n",
        "class TextGenerator(tf.keras.callbacks.Callback):\n",
        "    \"\"\"A callback to generate text from a trained model.\n",
        "    1. Feed some starting prompt to the model\n",
        "    2. Predict probabilities for the next token\n",
        "    3. Sample the next token and add it to the next input\n",
        "\n",
        "    Arguments:\n",
        "        max_tokens: Integer, the number of tokens to be generated after prompt.\n",
        "        start_tokens: List of integers, the token indices for the starting prompt.\n",
        "        index_to_word: List of strings, obtained from the TextVectorization layer.\n",
        "        top_k: Integer, sample from the `top_k` token predictions.\n",
        "        print_every: Integer, print after this many epochs.\n",
        "    \"\"\"\n",
        "\n",
        "    # Constructor function\n",
        "    def __init__(self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1):\n",
        "        \n",
        "        # Initialization\n",
        "        self.max_tokens = max_tokens\n",
        "        self.start_tokens = start_tokens\n",
        "        self.index_to_word = index_to_word\n",
        "        self.print_every = print_every\n",
        "        self.k = top_k\n",
        "\n",
        "    # Function for sampling from the model\n",
        "    def sample_from(self, logits):\n",
        "        \n",
        "        # Finds values and indices of the k largest entries for the last dimension.\n",
        "        logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n",
        "        \n",
        "        # Covert indices to numpy array\n",
        "        indices = np.asarray(indices).astype(\"int32\")\n",
        "        \n",
        "        # Softmax to convert logits to probabilities\n",
        "        preds = tf.keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
        "        \n",
        "        # Convert to numpy array\n",
        "        preds = np.asarray(preds).astype(\"float32\")\n",
        "        \n",
        "        # Generates a random sample from a given 1-D array\n",
        "        out = np.random.choice(indices, p=preds)\n",
        "        \n",
        "        return out \n",
        "\n",
        "    # Function for converting indices to tokens\n",
        "    def detokenize(self, number):\n",
        "        \n",
        "        # Convert index to word\n",
        "        return self.index_to_word[number]\n",
        "\n",
        "    # Function for generating text\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        \n",
        "        # Initialize the start tokens \n",
        "        start_tokens = [_ for _ in self.start_tokens]\n",
        "        \n",
        "        # Every `print_every` epochs\n",
        "        if (epoch + 1) % self.print_every != 0:\n",
        "            \n",
        "            # Return\n",
        "            return\n",
        "        \n",
        "        # Initialize the number of tokens generated\n",
        "        num_tokens_generated = 0\n",
        "        \n",
        "        # Initialize the tokens generated\n",
        "        tokens_generated = []\n",
        "        \n",
        "        # Loop until the number of tokens generated is less than the maximum number of tokens\n",
        "        while num_tokens_generated <= self.max_tokens:\n",
        "            \n",
        "            # Pad length \n",
        "            pad_len = maxlen - len(start_tokens)\n",
        "            \n",
        "            # The index of the last token in the start_tokens\n",
        "            sample_index = len(start_tokens) - 1\n",
        "            \n",
        "            # If the pad length is less than 0\n",
        "            if pad_len < 0:\n",
        "                \n",
        "                # Inputs: Start tokens from 0 to maxlen\n",
        "                x = start_tokens[:maxlen]\n",
        "                \n",
        "                # Set the sample index to maxlen - 1\n",
        "                sample_index = maxlen - 1\n",
        "                \n",
        "            # If the pad length is greater than 0\n",
        "            elif pad_len > 0:\n",
        "                \n",
        "                # Inputs: Start tokens and pad with 0s\n",
        "                x = start_tokens + [0] * pad_len\n",
        "                \n",
        "            # If the pad length is 0\n",
        "            else:\n",
        "                \n",
        "                # Inputs: Start tokens\n",
        "                x = start_tokens\n",
        "                \n",
        "            # Convert to numpy array\n",
        "            x = np.array([x])\n",
        "            \n",
        "            # Predict\n",
        "            y, _ = self.model.predict(x)\n",
        "            \n",
        "            # Sample from the model\n",
        "            sample_token = self.sample_from(y[0][sample_index])\n",
        "            \n",
        "            # Append the sample token to the generated tokens\n",
        "            tokens_generated.append(sample_token)\n",
        "            \n",
        "            # Append the sample token to the start tokens\n",
        "            start_tokens.append(sample_token)\n",
        "            \n",
        "            # Increment the number of tokens generated\n",
        "            num_tokens_generated = len(tokens_generated)\n",
        "            \n",
        "        # Join the predicted tokens \n",
        "        txt = \" \".join([self.detokenize(_) for _ in self.start_tokens + tokens_generated])\n",
        "        \n",
        "        # Report\n",
        "        print(f\"GENERATED TEXT:\\n{txt}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize a word2index dictionary\n",
        "word_to_index = {}\n",
        "\n",
        "# Loop over the vocabulary\n",
        "for index, word in enumerate(vocab):\n",
        "    \n",
        "    # Add word and index to the dictionary\n",
        "    word_to_index[word] = index\n",
        "\n",
        "# Start prompt\n",
        "start_prompt = \"the wizard \"\n",
        "\n",
        "# Convert the start prompt to token indices\n",
        "start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n",
        "\n",
        "# Number of tokens to be generated\n",
        "num_tokens_generated = 50\n",
        "\n",
        "# Initialize the text generator\n",
        "text_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yMqEqaKDGhYG"
      },
      "source": [
        "<br>\n",
        "\n",
        "### TRAINING\n",
        "\n",
        "---\n",
        "\n",
        "Note: This code should preferably be run on GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 80)]              0         \n",
            "_________________________________________________________________\n",
            "token_and_position_embedding (None, 80, 256)           5140480   \n",
            "_________________________________________________________________\n",
            "transformer_block (Transform (None, 80, 256)           658688    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 80, 20000)         5140000   \n",
            "=================================================================\n",
            "Total params: 10,939,168\n",
            "Trainable params: 10,939,168\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Initialize the model\n",
        "model = create_model()\n",
        "\n",
        "# Model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "n3_5sUpJGhYH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "391/391 [==============================] - 49s 117ms/step - loss: 5.5732 - dense_2_loss: 5.5732\n",
            "GENERATED TEXT:\n",
            "the wizard is a [UNK] , the only to make -up [UNK] of [UNK] [UNK] ' , [UNK] 's [UNK] , \" [UNK] . [UNK] ) , [UNK] , [UNK] [UNK] in the [UNK] , \" , [UNK] ) , the [UNK] [UNK] and [UNK] of [UNK] 's [UNK] of all the best\n",
            "\n",
            "Epoch 2/30\n",
            "391/391 [==============================] - 50s 129ms/step - loss: 4.7059 - dense_2_loss: 4.7059\n",
            "GENERATED TEXT:\n",
            "the wizard of [UNK] in the first time , is [UNK] and a perfect example of [UNK] to a group of friends . [UNK] is the only one . i love the show is not one of the funniest films i 've ever seen , it 's just about [UNK] , but i\n",
            "\n",
            "Epoch 3/30\n",
            "391/391 [==============================] - 61s 156ms/step - loss: 4.4589 - dense_2_loss: 4.4589\n",
            "GENERATED TEXT:\n",
            "the wizard is a good looking [UNK] , but this is just a great deal of time . it takes place on the surface . this movie has some very good things in . the film is about two hours of your family . i love this flick that i was expecting much\n",
            "\n",
            "Epoch 4/30\n",
            "391/391 [==============================] - 71s 182ms/step - loss: 4.3018 - dense_2_loss: 4.3018\n",
            "GENERATED TEXT:\n",
            "the wizard was [UNK] and had been the [UNK] was a big disappointment . it was a very funny film i was pleasantly surprised . the story was told of a very interesting story . the story of a group of teenagers , who were the people who were talking to the movie\n",
            "\n",
            "Epoch 5/30\n",
            "391/391 [==============================] - 79s 201ms/step - loss: 4.1811 - dense_2_loss: 4.1811\n",
            "GENERATED TEXT:\n",
            "the wizard of oz is a great little [UNK] [UNK] . . . . . . . and if you are a fan of horror and have a good taste of films , you 've come to a great deal with it , but not that you are not so much fun ,\n",
            "\n",
            "Epoch 6/30\n",
            "391/391 [==============================] - 84s 216ms/step - loss: 4.0814 - dense_2_loss: 4.0814\n",
            "GENERATED TEXT:\n",
            "the wizard of [UNK] , this movie was one of the best movies ever made . [UNK] was just about an hour in my life and i loved this movie . i don 't think it was just the same as well as it went on , i decided to watch it and\n",
            "\n",
            "Epoch 7/30\n",
            "391/391 [==============================] - 88s 225ms/step - loss: 3.9962 - dense_2_loss: 3.9962\n",
            "GENERATED TEXT:\n",
            "the wizard of [UNK] in the [UNK] but the [UNK] \" is the only one that takes place to get a lot out of the world around . this was the last two hour and a half hour and a half dozen . the plot was very slow and confusing and the acting\n",
            "\n",
            "Epoch 8/30\n",
            "391/391 [==============================] - 94s 239ms/step - loss: 3.9217 - dense_2_loss: 3.9217\n",
            "GENERATED TEXT:\n",
            "the wizard of oz is an elvira [UNK] [UNK] movie with her piggy [UNK] [UNK] \" , and kermit , piggy , kermit , kermit the frog , kermit , piggy , kermit the frog frog , [UNK] piggy , [UNK] , piggy and kermit , gonzo , gonzo and [UNK] , the\n",
            "\n",
            "Epoch 9/30\n",
            "391/391 [==============================] - 94s 240ms/step - loss: 3.8555 - dense_2_loss: 3.8555\n",
            "GENERATED TEXT:\n",
            "the wizard was not a big star in 1968 , but this movie is the one of the best and most [UNK] movie ever made . i was very moved , and disappointed . i think its way that it was [UNK] from the first one and the few years . the actors\n",
            "\n",
            "Epoch 10/30\n",
            "391/391 [==============================] - 96s 246ms/step - loss: 3.7965 - dense_2_loss: 3.7965\n",
            "GENERATED TEXT:\n",
            "the wizard is not a [UNK] girl \" , but she 's always a pleasure to see her in bed , but it is one of those kind of movies that you can 't see this movie unless you [UNK] . this is your typical hollywood fare - the [UNK] of pop culture\n",
            "\n",
            "Epoch 11/30\n",
            "391/391 [==============================] - 106s 271ms/step - loss: 3.7425 - dense_2_loss: 3.7425\n",
            "GENERATED TEXT:\n",
            "the wizard is , [UNK] . it is my first experience for me and the [UNK] . it may have been a great movie for me . the first movie is a great [UNK] \" type thing is that you have seen in my life . . . . . . . .\n",
            "\n",
            "Epoch 12/30\n",
            "391/391 [==============================] - 100s 255ms/step - loss: 3.6936 - dense_2_loss: 3.6936\n",
            "GENERATED TEXT:\n",
            "the wizard tells of prince [UNK] [UNK] , a prince [UNK] , [UNK] [UNK] , which was a wonderful little too little too long . but , i didn 't understand that it was hard to imagine that the [UNK] of the [UNK] \" was a good film . this [UNK] was a\n",
            "\n",
            "Epoch 13/30\n",
            "391/391 [==============================] - 108s 277ms/step - loss: 3.6493 - dense_2_loss: 3.6493\n",
            "GENERATED TEXT:\n",
            "the wizard is [UNK] is [UNK] that he is a [UNK] man who is a woman who is married in [UNK] [UNK] ) and [UNK] , is about an elderly woman , who becomes suspected of murder . he is the [UNK] by the [UNK] of a local count von bruno hermann who\n",
            "\n",
            "Epoch 14/30\n",
            "391/391 [==============================] - 108s 275ms/step - loss: 3.6086 - dense_2_loss: 3.6086\n",
            "GENERATED TEXT:\n",
            "the wizard is [UNK] , a [UNK] boy [UNK] girl \" is a very cute [UNK] , but she is the only movie i have ever seen . the movie is [UNK] \" a bit of fluff and it 's not for me it . it is not a great character that you\n",
            "\n",
            "Epoch 15/30\n",
            "391/391 [==============================] - 105s 267ms/step - loss: 3.5709 - dense_2_loss: 3.5709\n",
            "GENERATED TEXT:\n",
            "the wizard tells of oz , you know the film , but you know that it 's about a young girl named joe [UNK] \" [UNK] . she is an [UNK] man and a [UNK] girl \" who is [UNK] from a small village where young girl fall in love , but when\n",
            "\n",
            "Epoch 16/30\n",
            "391/391 [==============================] - 104s 267ms/step - loss: 3.5360 - dense_2_loss: 3.5360\n",
            "GENERATED TEXT:\n",
            "the wizard is one of [UNK] boy 's [UNK] , but the film is that he has to be the best of [UNK] and most people in the world of film , i am not in america . in the same time he can 't make it . he 's an old man\n",
            "\n",
            "Epoch 17/30\n",
            "391/391 [==============================] - 106s 271ms/step - loss: 3.5036 - dense_2_loss: 3.5036\n",
            "GENERATED TEXT:\n",
            "the wizard of [UNK] [UNK] , mr . [UNK] [UNK] \" [UNK] , [UNK] [UNK] , [UNK] and [UNK] . \" i was so disappointed in the last half hearted \"golden years \" and out -of [UNK] \" and \"the wizard of oz \" , you know the wizard of oz , [UNK]\n",
            "\n",
            "Epoch 18/30\n",
            "391/391 [==============================] - 107s 275ms/step - loss: 3.4741 - dense_2_loss: 3.4741\n",
            "GENERATED TEXT:\n",
            "the wizard is that [UNK] is [UNK] [UNK] . i 've been [UNK] by many of mr . wizard . but [UNK] . oz is one of my favorite [UNK] [UNK] [UNK] . . [UNK] and [UNK] [UNK] . . . [UNK] . .i . . . \" . . . . .\n",
            "\n",
            "Epoch 19/30\n",
            "391/391 [==============================] - 106s 271ms/step - loss: 3.4457 - dense_2_loss: 3.4457\n",
            "GENERATED TEXT:\n",
            "the wizard of oz is one of oz -like film 's best . oz , and [UNK] . oz was it 's not too much . it 's a film about an abused woman approaching television station , who becomes the case of oz , as a teenage witch with the fact that\n",
            "\n",
            "Epoch 20/30\n",
            "391/391 [==============================] - 112s 287ms/step - loss: 3.4202 - dense_2_loss: 3.4202\n",
            "GENERATED TEXT:\n",
            "the wizard of oz starts as an attempt to be a comedy in oz , but it turns out to be an [UNK] , as a sentimental soap opera . the movie was a bit more like a soft porn star , but then you get a kick -ass , a girl whose\n",
            "\n",
            "Epoch 21/30\n",
            "391/391 [==============================] - 114s 292ms/step - loss: 3.3962 - dense_2_loss: 3.3962\n",
            "GENERATED TEXT:\n",
            "the wizard of oz starts in a brooklyn [UNK] prince [UNK] of pop -culture franchise which has become mixed in with [UNK] , [UNK] [UNK] of a group of young children who decide to pick it up to kill . it is a [UNK] that has become infected by one of the [UNK]\n",
            "\n",
            "Epoch 22/30\n",
            "391/391 [==============================] - 102s 260ms/step - loss: 3.3732 - dense_2_loss: 3.3732\n",
            "GENERATED TEXT:\n",
            "the wizard of [UNK] \" in 1979 was a [UNK] man in 1934 ) sitcom successful , as a british [UNK] \" type of persona as a producer who [UNK] the [UNK] [UNK] of [UNK] [UNK] \" or [UNK] [UNK] , which was a [UNK] actor , the [UNK] \" was originally a\n",
            "\n",
            "Epoch 23/30\n",
            "391/391 [==============================] - 101s 258ms/step - loss: 3.3522 - dense_2_loss: 3.3522\n",
            "GENERATED TEXT:\n",
            "the wizard of [UNK] of [UNK] , [UNK] [UNK] (as pony ) , the [UNK] bridge , the [UNK] [UNK] [UNK] 's [UNK] \" and [UNK] sunshine ) , a [UNK] prince [UNK] , a new [UNK] film that has been lost . it is a very good actor in chicago , a\n",
            "\n",
            "Epoch 24/30\n",
            "391/391 [==============================] - 100s 255ms/step - loss: 3.3318 - dense_2_loss: 3.3318\n",
            "GENERATED TEXT:\n",
            "the wizard of [UNK] , [UNK] , \" and i would have liked to be a [UNK] for the most part . if you 're in a young man . i don 't have to say it , you 've never been a fan of a bad movie . .   \n",
            "\n",
            "Epoch 25/30\n",
            "391/391 [==============================] - 107s 274ms/step - loss: 3.3127 - dense_2_loss: 3.3127\n",
            "GENERATED TEXT:\n",
            "the wizard of this [UNK] boy [UNK] (as joe [UNK] ) , a young lady macbeth who was born . the town of [UNK] \" is a young woman , who takes a new champion of town in the [UNK] . the title character of the town , the [UNK] , is [UNK]\n",
            "\n",
            "Epoch 26/30\n",
            "391/391 [==============================] - 111s 283ms/step - loss: 3.2943 - dense_2_loss: 3.2943\n",
            "GENERATED TEXT:\n",
            "the wizard of oz is one of oz films that could be considered a man that was the funniest and most forgotten movies i have seen , because it is so funny that the only thing that distinguishes it for me from being a decade of 1971 , and was also a bit\n",
            "\n",
            "Epoch 27/30\n",
            "391/391 [==============================] - 112s 286ms/step - loss: 3.2779 - dense_2_loss: 3.2779\n",
            "GENERATED TEXT:\n",
            "the wizard of oz is a [UNK] prince who learns love to grow up and enjoy the [UNK] \" and [UNK] . it 's not worth the wait for [UNK] \" to see it . the film is about a young man named [UNK] , his declaration to japan [UNK] city . his\n",
            "\n",
            "Epoch 28/30\n",
            "391/391 [==============================] - 109s 278ms/step - loss: 3.2616 - dense_2_loss: 3.2616\n",
            "GENERATED TEXT:\n",
            "the wizard of oz starts in oz , locke and eko follow in the \" episode , but the rest of the movie is about the family , locke is [UNK] , as the lock legs , two boys and the sisters have their sisters but the family members of the island in\n",
            "\n",
            "Epoch 29/30\n",
            "391/391 [==============================] - 108s 276ms/step - loss: 3.2463 - dense_2_loss: 3.2463\n",
            "GENERATED TEXT:\n",
            "the wizard of oz was one of the last [UNK] [UNK] man , this show for you who was the most memorable of her silent era . this show is , and it still holds your attention and it 's own , but this is a great movie . the acting was good\n",
            "\n",
            "Epoch 30/30\n",
            "391/391 [==============================] - 110s 281ms/step - loss: 3.2319 - dense_2_loss: 3.2319\n",
            "GENERATED TEXT:\n",
            "the wizard is , i would give this movie a 10 and it is a little better . i was right , but this movie was not a good one . and even the plot was very simple . it is a good film that it is about a [UNK] [UNK] named alice\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x25cf2262700>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the model\n",
        "model.fit(text_ds, verbose=1, epochs=30, callbacks=[text_gen_callback])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>\n",
        "\n",
        "### PREDICTION\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Class for generating text\n",
        "# class TextGenerator():\n",
        "\n",
        "#     # Constructor function\n",
        "#     def __init__(self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1):\n",
        "        \n",
        "#         # Initialization\n",
        "#         self.max_tokens = max_tokens\n",
        "#         self.start_tokens = start_tokens\n",
        "#         self.index_to_word = index_to_word\n",
        "#         self.print_every = print_every\n",
        "#         self.k = top_k\n",
        "\n",
        "#     # Function for sampling from the model\n",
        "#     def sample_from(self, logits):\n",
        "        \n",
        "#         # Finds values and indices of the k largest entries for the last dimension.\n",
        "#         logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n",
        "        \n",
        "#         # Covert indices to numpy array\n",
        "#         indices = np.asarray(indices).astype(\"int32\")\n",
        "        \n",
        "#         # Softmax to convert logits to probabilities\n",
        "#         preds = tf.keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
        "        \n",
        "#         # Convert to numpy array\n",
        "#         preds = np.asarray(preds).astype(\"float32\")\n",
        "        \n",
        "#         # Generates a random sample from a given 1-D array\n",
        "#         out = np.random.choice(indices, p=preds)\n",
        "        \n",
        "#         return out \n",
        "\n",
        "#     # Function for converting indices to tokens\n",
        "#     def detokenize(self, number):\n",
        "        \n",
        "#         # Convert index to word\n",
        "#         return self.index_to_word[number]\n",
        "\n",
        "#     # Function for generating text\n",
        "#     def on_epoch_end(self, epoch, logs=None):\n",
        "        \n",
        "#         # Initialize the start tokens \n",
        "#         start_tokens = [_ for _ in self.start_tokens]\n",
        "        \n",
        "#         # Every `print_every` epochs\n",
        "#         if (epoch + 1) % self.print_every != 0:\n",
        "            \n",
        "#             # Return\n",
        "#             return\n",
        "        \n",
        "#         # Initialize the number of tokens generated\n",
        "#         num_tokens_generated = 0\n",
        "        \n",
        "#         # Initialize the tokens generated\n",
        "#         tokens_generated = []\n",
        "        \n",
        "#         # Loop until the number of tokens generated is less than the maximum number of tokens\n",
        "#         while num_tokens_generated <= self.max_tokens:\n",
        "            \n",
        "#             # Pad length \n",
        "#             pad_len = maxlen - len(start_tokens)\n",
        "            \n",
        "#             # The index of the last token in the start_tokens\n",
        "#             sample_index = len(start_tokens) - 1\n",
        "            \n",
        "#             # If the pad length is less than 0\n",
        "#             if pad_len < 0:\n",
        "                \n",
        "#                 # Inputs: Start tokens from 0 to maxlen\n",
        "#                 x = start_tokens[:maxlen]\n",
        "                \n",
        "#                 # Set the sample index to maxlen - 1\n",
        "#                 sample_index = maxlen - 1\n",
        "                \n",
        "#             # If the pad length is greater than 0\n",
        "#             elif pad_len > 0:\n",
        "                \n",
        "#                 # Inputs: Start tokens and pad with 0s\n",
        "#                 x = start_tokens + [0] * pad_len\n",
        "                \n",
        "#             # If the pad length is 0\n",
        "#             else:\n",
        "                \n",
        "#                 # Inputs: Start tokens\n",
        "#                 x = start_tokens\n",
        "                \n",
        "#             # Convert to numpy array\n",
        "#             x = np.array([x])\n",
        "            \n",
        "#             # Predict\n",
        "#             y, _ = self.model.predict(x)\n",
        "            \n",
        "#             # Sample from the model\n",
        "#             sample_token = self.sample_from(y[0][sample_index])\n",
        "            \n",
        "#             # Append the sample token to the generated tokens\n",
        "#             tokens_generated.append(sample_token)\n",
        "            \n",
        "#             # Append the sample token to the start tokens\n",
        "#             start_tokens.append(sample_token)\n",
        "            \n",
        "#             # Increment the number of tokens generated\n",
        "#             num_tokens_generated = len(tokens_generated)\n",
        "            \n",
        "#         # Join the predicted tokens \n",
        "#         txt = \" \".join([self.detokenize(_) for _ in self.start_tokens + tokens_generated])\n",
        "        \n",
        "#         # Report\n",
        "#         print(f\"GENERATED TEXT:\\n{txt}\\n\")\n",
        "\n",
        "# # Initialize a word2index dictionary\n",
        "# word_to_index = {}\n",
        "\n",
        "# # Loop over the vocabulary\n",
        "# for index, word in enumerate(vocab):\n",
        "    \n",
        "#     # Add word and index to the dictionary\n",
        "#     word_to_index[word] = index\n",
        "\n",
        "# # Start prompt\n",
        "# start_prompt = \"the wizard \"\n",
        "\n",
        "# # Convert the start prompt to token indices\n",
        "# start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n",
        "\n",
        "# # Number of tokens to be generated\n",
        "# num_tokens_generated = 50\n",
        "\n",
        "# # Initialize the text generator\n",
        "# text_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>\n",
        "\n",
        "### EVALUATION\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "text_generation_with_miniature_gpt",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "c992f0b96e0bf1b7bf4ed1da6ffb7bb064b2bb3e7201b712e1347652797d5077"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
