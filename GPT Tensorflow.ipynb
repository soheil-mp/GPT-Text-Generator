{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mXEJ-Tp_GhXl"
      },
      "source": [
        "<br>\n",
        "\n",
        "<h1 style=\"text-align:center;\">Text generation with a miniature GPT</h1>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pJbw_wByGhXw"
      },
      "source": [
        "<br>\n",
        "\n",
        "### Introduction\n",
        "\n",
        "---\n",
        "\n",
        "Implementation of an autoregressive language model using the GPT model.\n",
        " \n",
        "We use the IMDB sentiment classification dataset for training generate new movie reviews for a given prompt.\n",
        "\n",
        "[GPT](https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035),\n",
        "[GPT-2](https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe),\n",
        "[GPT-3](https://arxiv.org/abs/2005.14165)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq62fG8qGhXy"
      },
      "source": [
        "<br>\n",
        "\n",
        "### INITIAL SETUP\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JdKgIbOSGhXz"
      },
      "outputs": [],
      "source": [
        "# Import the libraries\n",
        "import re, os, string, random\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2J2Al3I-GhX2"
      },
      "source": [
        "<br>\n",
        "\n",
        "### TRANSFORMER BLOCK\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Causal attention mask function\n",
        "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
        "    \"\"\"\n",
        "    This function creates a causal attention mask for the transformer model.\n",
        "    More specifically, it will mask the upper half of the dot product matrix in \n",
        "    self attention (to prevent flow of information from future tokens to current).\n",
        "    ARGUMENTS\n",
        "    =================\n",
        "        - batch_size: batch size of the input\n",
        "        - n_dest: number of tokens in the destination sequence\n",
        "        - n_src: number of tokens in the source sequence\n",
        "        - dtype: data type of the mask\n",
        "        \n",
        "    RETURNS\n",
        "    =================\n",
        "        - out: causal attention mask\n",
        "    \"\"\"\n",
        "    \n",
        "    # Initialize the indices\n",
        "    i = tf.range(n_dest)[:, None]\n",
        "    j = tf.range(n_src)\n",
        "    \n",
        "    # Create the mask \n",
        "    m = i >= j - n_src + n_dest\n",
        "    \n",
        "    # Cast the mask\n",
        "    mask = tf.cast(m, dtype)\n",
        "    \n",
        "    # Expand the mask\n",
        "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
        "    \n",
        "    # This is the mask that we will use to mask the upper half of the dot product matrix\n",
        "    mult = tf.concat([tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0)\n",
        "    \n",
        "    # Tile the mask \n",
        "    out = tf.tile(mask, mult)\n",
        "    \n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lTozsQRkGhX3"
      },
      "outputs": [],
      "source": [
        "# Transformer block class\n",
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    \n",
        "    # Initialize the constructor\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        \n",
        "        # Inherit the parent's constructor\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        \n",
        "        # Multi-head attention layer\n",
        "        self.att = tf.keras.layers.MultiHeadAttention(num_heads, embed_dim)\n",
        "        \n",
        "        # Feed forward network\n",
        "        self.ffn = tf.keras.Sequential(\n",
        "            [tf.keras.layers.Dense(ff_dim, activation=\"relu\"), tf.keras.layers.Dense(embed_dim),]\n",
        "        )\n",
        "        \n",
        "        # Layer normalization\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        # Dropout regularization\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    # Call function\n",
        "    def call(self, inputs):\n",
        "        \n",
        "        # Input shape\n",
        "        input_shape = tf.shape(inputs)\n",
        "        \n",
        "        # Batch size\n",
        "        batch_size = input_shape[0]\n",
        "        \n",
        "        # Sequence length\n",
        "        seq_len = input_shape[1]\n",
        "        \n",
        "        # Causal mask\n",
        "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
        "        \n",
        "        # Multi-head attention with causal mask\n",
        "        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
        "        \n",
        "        # Dropout regularization\n",
        "        attention_output = self.dropout1(attention_output)\n",
        "        \n",
        "        # Add and normalize layers\n",
        "        out1 = self.layernorm1(inputs + attention_output)\n",
        "        \n",
        "        # Feed forward network\n",
        "        ffn_output = self.ffn(out1)\n",
        "        \n",
        "        # Dropout regularization\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        \n",
        "        # Add and normalize layers\n",
        "        out = self.layernorm2(out1 + ffn_output)\n",
        "        \n",
        "        return out\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OL5kjkJ6GhX5"
      },
      "source": [
        "<br>\n",
        "\n",
        "### EMBEDDING LAYER\n",
        "\n",
        "---\n",
        "\n",
        "Create two seperate embedding layers: one for tokens and one for token index\n",
        "(positions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NqgmSDoSGhX6"
      },
      "outputs": [],
      "source": [
        "# Token and position embedding class\n",
        "class TokenAndPositionEmbedding(tf.keras.layers.Layer):\n",
        "    \n",
        "    # Initialize the constructor\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        \n",
        "        # Inherit the parent's constructor\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        \n",
        "        # Token embedding layer\n",
        "        self.token_emb = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        \n",
        "        # Position embedding layer\n",
        "        self.pos_emb = tf.keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    # Call function\n",
        "    def call(self, x):\n",
        "        \n",
        "        # Maximum sequence length\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        \n",
        "        # Initialize the positions\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        \n",
        "        # Feed the positions to the position embedding layer\n",
        "        positions = self.pos_emb(positions)\n",
        "        \n",
        "        # Feed the tokens to the token embedding layer\n",
        "        x = self.token_emb(x)\n",
        "        \n",
        "        # Add the token and position embeddings\n",
        "        out = x + positions\n",
        "        \n",
        "        return out"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mpcEw9Y5GhX8"
      },
      "source": [
        "<br>\n",
        "\n",
        "### GPT model\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialization\n",
        "vocab_size = 20000       # Vocabulary (only consider the top 20k words)\n",
        "maxlen = 80              # Max sequence size\n",
        "embed_dim = 256          # Embedding size for each token\n",
        "num_heads = 2            # Number of attention heads\n",
        "feed_forward_dim = 256   # Hidden layer size in feed forward network inside transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "X-A69BerGhX9"
      },
      "outputs": [],
      "source": [
        "# GPT model\n",
        "def create_model():\n",
        "    \n",
        "    # Input layer\n",
        "    inputs = tf.keras.layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
        "    \n",
        "    # Token and position embedding layer\n",
        "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "    x = embedding_layer(inputs)\n",
        "    \n",
        "    # Transformer block\n",
        "    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
        "    x = transformer_block(x)\n",
        "    \n",
        "    # Output layer\n",
        "    outputs = tf.keras.layers.Dense(vocab_size)(x)\n",
        "    \n",
        "    # Construct the model\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=[outputs, x])\n",
        "    \n",
        "    # Loss function\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    \n",
        "    # Compile the model\n",
        "    model.compile(\"adam\", loss=[loss_fn, None],)  # No loss and optimization based on word embeddings from transformer block\n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8nr2u-FtGhX_"
      },
      "source": [
        "<br>\n",
        "\n",
        "### DOWNLOAD AND PREPARE DATASET\n",
        "\n",
        "---\n",
        "\n",
        "Download the IMDB dataset and combine training and validation sets for a text\n",
        "generation task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pq7ZGuFxGhYA"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0 80.2M    0 49152    0     0  32751      0  0:42:48  0:00:01  0:42:47 32768\n",
            "  0 80.2M    0  560k    0     0   222k      0  0:06:09  0:00:02  0:06:07  222k\n",
            "  4 80.2M    4 3824k    0     0  1083k      0  0:01:15  0:00:03  0:01:12 1083k\n",
            "  6 80.2M    6 5056k    0     0  1093k      0  0:01:15  0:00:04  0:01:11 1094k\n",
            "  8 80.2M    8 7184k    0     0  1307k      0  0:01:02  0:00:05  0:00:57 1431k\n",
            "  9 80.2M    9 7952k    0     0  1233k      0  0:01:06  0:00:06  0:01:00 1597k\n",
            " 15 80.2M   15 12.5M    0     0  1720k      0  0:00:47  0:00:07  0:00:40 2483k\n",
            " 19 80.2M   19 15.6M    0     0  1812k      0  0:00:45  0:00:08  0:00:37 2300k\n",
            " 19 80.2M   19 15.6M    0     0  1626k      0  0:00:50  0:00:09  0:00:41 2099k\n",
            " 19 80.2M   19 15.6M    0     0  1525k      0  0:00:53  0:00:10  0:00:43 1764k\n",
            " 21 80.2M   21 17.5M    0     0  1559k      0  0:00:52  0:00:11  0:00:41 1972k\n",
            " 26 80.2M   26 21.3M    0     0  1739k      0  0:00:47  0:00:12  0:00:35 1767k\n",
            " 32 80.2M   32 26.3M    0     0  2005k      0  0:00:40  0:00:13  0:00:27 2370k\n",
            " 37 80.2M   37 30.4M    0     0  2156k      0  0:00:38  0:00:14  0:00:24 3283k\n",
            " 42 80.2M   42 34.0M    0     0  2254k      0  0:00:36  0:00:15  0:00:21 3812k\n",
            " 47 80.2M   47 38.2M    0     0  2382k      0  0:00:34  0:00:16  0:00:18 4302k\n",
            " 54 80.2M   54 43.6M    0     0  2556k      0  0:00:32  0:00:17  0:00:15 4651k\n",
            " 61 80.2M   61 49.0M    0     0  2721k      0  0:00:30  0:00:18  0:00:12 4649k\n",
            " 67 80.2M   67 54.5M    0     0  2869k      0  0:00:28  0:00:19  0:00:09 4929k\n",
            " 74 80.2M   74 60.0M    0     0  3004k      0  0:00:27  0:00:20  0:00:07 5320k\n",
            " 81 80.2M   81 65.5M    0     0  3125k      0  0:00:26  0:00:21  0:00:05 5574k\n",
            " 88 80.2M   88 70.9M    0     0  3235k      0  0:00:25  0:00:22  0:00:03 5618k\n",
            " 94 80.2M   94 76.1M    0     0  3324k      0  0:00:24  0:00:23  0:00:01 5544k\n",
            "100 80.2M  100 80.2M    0     0  3393k      0  0:00:24  0:00:24 --:--:-- 5535k\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset\n",
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Batch size\n",
        "batch_size = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50000 files\n"
          ]
        }
      ],
      "source": [
        "# Initialize a list for filesnames\n",
        "filenames = []\n",
        "\n",
        "# Loop over the directories\n",
        "for dir in [\"aclImdb/train/pos\", \"aclImdb/train/neg\", \"aclImdb/test/pos\", \"aclImdb/test/neg\"]:\n",
        "    \n",
        "    # Loop over the files inside each directory\n",
        "    for f in os.listdir(dir):\n",
        "        \n",
        "        # Append the filename to the list\n",
        "        filenames.append(os.path.join(dir, f))\n",
        "\n",
        "# Report\n",
        "print(f\"{len(filenames)} files\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Shuffle the filenames\n",
        "random.shuffle(filenames)\n",
        "\n",
        "# Load the dataset through tf.data\n",
        "text_ds = tf.data.TextLineDataset(filenames)\n",
        "\n",
        "# Shuffle the dataset\n",
        "text_ds = text_ds.shuffle(buffer_size=256)\n",
        "\n",
        "# Set the batch size\n",
        "text_ds = text_ds.batch(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wTe5Z5_IGhYB"
      },
      "outputs": [],
      "source": [
        "# Function for custom standardization\n",
        "def custom_standardization(input_string):\n",
        "    \n",
        "    # Lowercase the text\n",
        "    lowercased = tf.strings.lower(input_string)\n",
        "    \n",
        "    # Remove html line-break tags\n",
        "    stripped_html = tf.strings.regex_replace(lowercased, \"<br />\", \" \")\n",
        "    \n",
        "    # Handle punctuation\n",
        "    out = tf.strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n",
        "    \n",
        "    return out\n",
        "\n",
        "\n",
        "# Text vectorization layer\n",
        "vectorize_layer = tf.keras.layers.TextVectorization(standardize=custom_standardization,\n",
        "                                                    max_tokens=vocab_size - 1,\n",
        "                                                    output_mode=\"int\",\n",
        "                                                    output_sequence_length=maxlen + 1,\n",
        ")\n",
        "\n",
        "# Adapt the vectorization layer to the text\n",
        "vectorize_layer.adapt(text_ds)\n",
        "\n",
        "# Vocabulary list\n",
        "vocab = vectorize_layer.get_vocabulary()  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function for preparing the inputs and labels\n",
        "def prepare_lm_inputs_labels(text):\n",
        "    \n",
        "    # Add the extra dimension to the text\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    \n",
        "    # Vectorize the text\n",
        "    tokenized_sentences = vectorize_layer(text)\n",
        "    \n",
        "    # Inputs (all words except the last)\n",
        "    x = tokenized_sentences[:, :-1]\n",
        "    \n",
        "    # Labels (shifted one position)\n",
        "    y = tokenized_sentences[:, 1:]\n",
        "    \n",
        "    return x, y\n",
        "\n",
        "# Map the function to the dataset\n",
        "text_ds = text_ds.map(prepare_lm_inputs_labels)\n",
        "\n",
        "# Prefetch the dataset\n",
        "text_ds = text_ds.prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hIfgyag5GhYD"
      },
      "source": [
        "<br>\n",
        "\n",
        "### CALLBACK FUNCTION\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zcRD0dkkGhYE"
      },
      "outputs": [],
      "source": [
        "# Class for generating text\n",
        "class TextGenerator(tf.keras.callbacks.Callback):\n",
        "    \"\"\"A callback to generate text from a trained model.\n",
        "    1. Feed some starting prompt to the model\n",
        "    2. Predict probabilities for the next token\n",
        "    3. Sample the next token and add it to the next input\n",
        "\n",
        "    Arguments:\n",
        "        max_tokens: Integer, the number of tokens to be generated after prompt.\n",
        "        start_tokens: List of integers, the token indices for the starting prompt.\n",
        "        index_to_word: List of strings, obtained from the TextVectorization layer.\n",
        "        top_k: Integer, sample from the `top_k` token predictions.\n",
        "        print_every: Integer, print after this many epochs.\n",
        "    \"\"\"\n",
        "\n",
        "    # Constructor function\n",
        "    def __init__(self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1):\n",
        "        \n",
        "        # Initialization\n",
        "        self.max_tokens = max_tokens\n",
        "        self.start_tokens = start_tokens\n",
        "        self.index_to_word = index_to_word\n",
        "        self.print_every = print_every\n",
        "        self.k = top_k\n",
        "\n",
        "    # Function for sampling from the model\n",
        "    def sample_from(self, logits):\n",
        "        \n",
        "        # Finds values and indices of the k largest entries for the last dimension.\n",
        "        logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n",
        "        \n",
        "        # Covert indices to numpy array\n",
        "        indices = np.asarray(indices).astype(\"int32\")\n",
        "        \n",
        "        # Softmax to convert logits to probabilities\n",
        "        preds = tf.keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
        "        \n",
        "        # Convert to numpy array\n",
        "        preds = np.asarray(preds).astype(\"float32\")\n",
        "        \n",
        "        # Generates a random sample from a given 1-D array\n",
        "        out = np.random.choice(indices, p=preds)\n",
        "        \n",
        "        return out \n",
        "\n",
        "    # Function for converting indices to tokens\n",
        "    def detokenize(self, number):\n",
        "        \n",
        "        # Convert index to word\n",
        "        return self.index_to_word[number]\n",
        "\n",
        "    # Function for generating text\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        \n",
        "        # Initialize the start tokens \n",
        "        start_tokens = [_ for _ in self.start_tokens]\n",
        "        \n",
        "        # Every `print_every` epochs\n",
        "        if (epoch + 1) % self.print_every != 0:\n",
        "            \n",
        "            # Return\n",
        "            return\n",
        "        \n",
        "        # Initialize the number of tokens generated\n",
        "        num_tokens_generated = 0\n",
        "        \n",
        "        # Initialize the tokens generated\n",
        "        tokens_generated = []\n",
        "        \n",
        "        # Loop until the number of tokens generated is less than the maximum number of tokens\n",
        "        while num_tokens_generated <= self.max_tokens:\n",
        "            \n",
        "            # Pad length \n",
        "            pad_len = maxlen - len(start_tokens)\n",
        "            \n",
        "            # The index of the last token in the start_tokens\n",
        "            sample_index = len(start_tokens) - 1\n",
        "            \n",
        "            # If the pad length is less than 0\n",
        "            if pad_len < 0:\n",
        "                \n",
        "                # Inputs: Start tokens from 0 to maxlen\n",
        "                x = start_tokens[:maxlen]\n",
        "                \n",
        "                # Set the sample index to maxlen - 1\n",
        "                sample_index = maxlen - 1\n",
        "                \n",
        "            # If the pad length is greater than 0\n",
        "            elif pad_len > 0:\n",
        "                \n",
        "                # Inputs: Start tokens and pad with 0s\n",
        "                x = start_tokens + [0] * pad_len\n",
        "                \n",
        "            # If the pad length is 0\n",
        "            else:\n",
        "                \n",
        "                # Inputs: Start tokens\n",
        "                x = start_tokens\n",
        "                \n",
        "            # Convert to numpy array\n",
        "            x = np.array([x])\n",
        "            \n",
        "            # Predict\n",
        "            y, _ = self.model.predict(x)\n",
        "            \n",
        "            # Sample from the model\n",
        "            sample_token = self.sample_from(y[0][sample_index])\n",
        "            \n",
        "            # Append the sample token to the generated tokens\n",
        "            tokens_generated.append(sample_token)\n",
        "            \n",
        "            # Append the sample token to the start tokens\n",
        "            start_tokens.append(sample_token)\n",
        "            \n",
        "            # Increment the number of tokens generated\n",
        "            num_tokens_generated = len(tokens_generated)\n",
        "            \n",
        "        # Join the predicted tokens \n",
        "        txt = \" \".join([self.detokenize(_) for _ in self.start_tokens + tokens_generated])\n",
        "        \n",
        "        # Report\n",
        "        print(f\"GENERATED TEXT:\\n{txt}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize a word2index dictionary\n",
        "word_to_index = {}\n",
        "\n",
        "# Loop over the vocabulary\n",
        "for index, word in enumerate(vocab):\n",
        "    \n",
        "    # Add word and index to the dictionary\n",
        "    word_to_index[word] = index\n",
        "\n",
        "# Start prompt\n",
        "start_prompt = \"the wizard \"\n",
        "\n",
        "# Convert the start prompt to token indices\n",
        "start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n",
        "\n",
        "# Number of tokens to be generated\n",
        "num_tokens_generated = 50\n",
        "\n",
        "# Initialize the text generator\n",
        "text_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yMqEqaKDGhYG"
      },
      "source": [
        "<br>\n",
        "\n",
        "### TRAINING\n",
        "\n",
        "---\n",
        "\n",
        "Note: This code should preferably be run on GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 80)]              0         \n",
            "_________________________________________________________________\n",
            "token_and_position_embedding (None, 80, 256)           5140480   \n",
            "_________________________________________________________________\n",
            "transformer_block_2 (Transfo (None, 80, 256)           658688    \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 80, 20000)         5140000   \n",
            "=================================================================\n",
            "Total params: 10,939,168\n",
            "Trainable params: 10,939,168\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Initialize the model\n",
        "model = create_model()\n",
        "\n",
        "# Model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "n3_5sUpJGhYH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "391/391 [==============================] - 59s 148ms/step - loss: 5.5808 - dense_8_loss: 5.5808\n",
            "GENERATED TEXT:\n",
            "the wizard of the world is not . the worst of it was a good . the movie is just plain awful , the acting , i was really liked this was so bad and i am very funny at least one of the best , i have watched it . . i\n",
            "\n",
            "Epoch 2/30\n",
            "391/391 [==============================] - 78s 200ms/step - loss: 4.7130 - dense_8_loss: 4.7130\n",
            "GENERATED TEXT:\n",
            "the wizard is a bit of the film 's , but it 's so hard to describe what i think i 'd expect it , if it wasn 't too bad and it 's really bad . i have a lot of it to say it was very funny and i was the\n",
            "\n",
            "Epoch 3/30\n",
            "391/391 [==============================] - 83s 212ms/step - loss: 4.4696 - dense_8_loss: 4.4696\n",
            "GENERATED TEXT:\n",
            "the wizard of oz was a great film , but in it was an awesome way to see . i had never heard of it , i was very happy with the same old [UNK] . the film also shows that we can get their own rules for . the film is about\n",
            "\n",
            "Epoch 4/30\n",
            "391/391 [==============================] - 81s 206ms/step - loss: 4.3148 - dense_8_loss: 4.3148\n",
            "GENERATED TEXT:\n",
            "the wizard of oz is one of the best films i have seen in the past . i was very impressed with an [UNK] in the middle of [UNK] , this movie is so awful that it was so bad that i was . i would give it a 10 ! i 'm\n",
            "\n",
            "Epoch 5/30\n",
            "391/391 [==============================] - 81s 206ms/step - loss: 4.1968 - dense_8_loss: 4.1968\n",
            "GENERATED TEXT:\n",
            "the wizard is a pretty good example of a good story about a couple of a young child 's and a [UNK] who lives and the [UNK] [UNK] [UNK] ) . in the end , a [UNK] of [UNK] and [UNK] . the film is about the best of the show i 've\n",
            "\n",
            "Epoch 6/30\n",
            "391/391 [==============================] - 94s 239ms/step - loss: 4.0993 - dense_8_loss: 4.0993\n",
            "GENERATED TEXT:\n",
            "the wizard of oz , a boy , boy , and i really enjoyed it . i had to see the film with my friends and i must say that this is a movie . i have never seen a lot more than this one . i think i can 't remember the\n",
            "\n",
            "Epoch 7/30\n",
            "391/391 [==============================] - 96s 244ms/step - loss: 4.0151 - dense_8_loss: 4.0151\n",
            "GENERATED TEXT:\n",
            "the wizard is not always a movie that 's why i would not recommend this movie to me . i 've seen the first time , this movie is so stupid is so bad , i couldn 't even get to see the whole thing i was watching this movie when the movie\n",
            "\n",
            "Epoch 8/30\n",
            "391/391 [==============================] - 100s 256ms/step - loss: 3.9418 - dense_8_loss: 3.9418\n",
            "GENERATED TEXT:\n",
            "the wizard of oz 's show [UNK] ' show , is set in a rural town where they are going to do it , the [UNK] ' [UNK] and [UNK] of the [UNK] in the air force of the late 70s . it is like the other characters , and their own lives\n",
            "\n",
            "Epoch 9/30\n",
            "391/391 [==============================] - 97s 248ms/step - loss: 3.8764 - dense_8_loss: 3.8764\n",
            "GENERATED TEXT:\n",
            "the wizard of oz is a [UNK] and [UNK] who is the greatest love for a [UNK] \" of the film . this film is the same as a [UNK] in the [UNK] ' or at the same time , but the [UNK] is one of the most intelligent and more intelligent ,\n",
            "\n",
            "Epoch 10/30\n",
            "391/391 [==============================] - 99s 253ms/step - loss: 3.8176 - dense_8_loss: 3.8176\n",
            "GENERATED TEXT:\n",
            "the wizard of oz is , the second time i have been searching for this series on tv in a long way . it 's hard to tell that it 's an homage to james stewart and anthony mann with his own life as much [UNK] of a previously tapped in this one\n",
            "\n",
            "Epoch 11/30\n",
            "391/391 [==============================] - 111s 284ms/step - loss: 3.7649 - dense_8_loss: 3.7649\n",
            "GENERATED TEXT:\n",
            "the wizard of oz , the first installment of the [UNK] \" franchise starring [UNK] ventura ,a -team [UNK] vet ,a group of young sho [UNK] and a boy [UNK] named [UNK] [UNK] [UNK] [UNK] ) ,who lives his wife has a job with a [UNK] and is the case for a [UNK]\n",
            "\n",
            "Epoch 12/30\n",
            "391/391 [==============================] - 108s 276ms/step - loss: 3.7162 - dense_8_loss: 3.7162\n",
            "GENERATED TEXT:\n",
            "the wizard of oz is the best oz ever . oz is a very bad sitcom , full , full of clichÃ©s that it 's not the same [UNK] of [UNK] and [UNK] . \" is an insult to injury [UNK] [UNK] by a guy who is on the [UNK] and [UNK] \"\n",
            "\n",
            "Epoch 13/30\n",
            "391/391 [==============================] - 136s 347ms/step - loss: 3.6727 - dense_8_loss: 3.6727\n",
            "GENERATED TEXT:\n",
            "the wizard of oz , this show is a perfect combination of [UNK] , and [UNK] and [UNK] , the [UNK] to the [UNK] [UNK] of seven ' is a [UNK] . it is a [UNK] ' , a bit of a departure from [UNK] [UNK] who does a fine job with a\n",
            "\n",
            "Epoch 14/30\n",
            "391/391 [==============================] - 174s 445ms/step - loss: 3.6317 - dense_8_loss: 3.6317\n",
            "GENERATED TEXT:\n",
            "the wizard of oz is bereft of humor from the same old [UNK] humor , humor , substituting characters and humour . it was so real . it has the same story lines from beginning to make us grow . the story is not the best of your average triumph in the tragedy\n",
            "\n",
            "Epoch 15/30\n",
            "391/391 [==============================] - 174s 443ms/step - loss: 3.5951 - dense_8_loss: 3.5951\n",
            "GENERATED TEXT:\n",
            "the wizard of mars is a [UNK] [UNK] for a [UNK] [UNK] to [UNK] [UNK] , [UNK] , [UNK] & [UNK] , is an old -fashioned , [UNK] , and [UNK] of [UNK] \" [UNK] , [UNK] [UNK] , [UNK] [UNK] , [UNK] [UNK] , [UNK] , is the sort of a [UNK]\n",
            "\n",
            "Epoch 16/30\n",
            "391/391 [==============================] - 174s 446ms/step - loss: 3.5600 - dense_8_loss: 3.5600\n",
            "GENERATED TEXT:\n",
            "the wizard of oz is bereft of the same possession of a pretty good old [UNK] ; the old hand , the old lady and the tramp who keeps up of the millionaire playboy 's father 's son -in -law , but she also falls in love with him . the [UNK] [UNK]\n",
            "\n",
            "Epoch 17/30\n",
            "391/391 [==============================] - 162s 414ms/step - loss: 3.5282 - dense_8_loss: 3.5282\n",
            "GENERATED TEXT:\n",
            "the wizard of oz was bereft of a noisy [UNK] , where the [UNK] [UNK] noisy [UNK] ! \" slight gags ; \"quirky , cheaply . \" and corny and lame jokes , the plot is predictable and formulaic fare for those corny films , even if they 're not to mention bad\n",
            "\n",
            "Epoch 18/30\n",
            "391/391 [==============================] - 164s 419ms/step - loss: 3.4985 - dense_8_loss: 3.4985\n",
            "GENERATED TEXT:\n",
            "the wizard of los angeles has been [UNK] and [UNK] from [UNK] to the sea \" in the future in this movie , it 's a great [UNK] , but this is the [UNK] and [UNK] [UNK] , ' and [UNK] [UNK] , \" and [UNK] [UNK] . i think [UNK] , this\n",
            "\n",
            "Epoch 19/30\n",
            "391/391 [==============================] - 172s 440ms/step - loss: 3.4709 - dense_8_loss: 3.4709\n",
            "GENERATED TEXT:\n",
            "the wizard of sea is [UNK] a terrific performance as a young boy . a young boy is a [UNK] and a scientist , but a comet is going to climb up for one of the [UNK] . [UNK] is in his [UNK] . he plays a very well [UNK] [UNK] . [UNK]\n",
            "\n",
            "Epoch 20/30\n",
            " 14/391 [>.............................] - ETA: 2:27 - loss: 3.4427 - dense_8_loss: 3.4427"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "model.fit(text_ds, verbose=1, epochs=30, callbacks=[text_gen_callback])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>\n",
        "\n",
        "### PREDICTION\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Class for generating text\n",
        "# class TextGenerator():\n",
        "\n",
        "#     # Constructor function\n",
        "#     def __init__(self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1):\n",
        "        \n",
        "#         # Initialization\n",
        "#         self.max_tokens = max_tokens\n",
        "#         self.start_tokens = start_tokens\n",
        "#         self.index_to_word = index_to_word\n",
        "#         self.print_every = print_every\n",
        "#         self.k = top_k\n",
        "\n",
        "#     # Function for sampling from the model\n",
        "#     def sample_from(self, logits):\n",
        "        \n",
        "#         # Finds values and indices of the k largest entries for the last dimension.\n",
        "#         logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n",
        "        \n",
        "#         # Covert indices to numpy array\n",
        "#         indices = np.asarray(indices).astype(\"int32\")\n",
        "        \n",
        "#         # Softmax to convert logits to probabilities\n",
        "#         preds = tf.keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
        "        \n",
        "#         # Convert to numpy array\n",
        "#         preds = np.asarray(preds).astype(\"float32\")\n",
        "        \n",
        "#         # Generates a random sample from a given 1-D array\n",
        "#         out = np.random.choice(indices, p=preds)\n",
        "        \n",
        "#         return out \n",
        "\n",
        "#     # Function for converting indices to tokens\n",
        "#     def detokenize(self, number):\n",
        "        \n",
        "#         # Convert index to word\n",
        "#         return self.index_to_word[number]\n",
        "\n",
        "#     # Function for generating text\n",
        "#     def on_epoch_end(self, epoch, logs=None):\n",
        "        \n",
        "#         # Initialize the start tokens \n",
        "#         start_tokens = [_ for _ in self.start_tokens]\n",
        "        \n",
        "#         # Every `print_every` epochs\n",
        "#         if (epoch + 1) % self.print_every != 0:\n",
        "            \n",
        "#             # Return\n",
        "#             return\n",
        "        \n",
        "#         # Initialize the number of tokens generated\n",
        "#         num_tokens_generated = 0\n",
        "        \n",
        "#         # Initialize the tokens generated\n",
        "#         tokens_generated = []\n",
        "        \n",
        "#         # Loop until the number of tokens generated is less than the maximum number of tokens\n",
        "#         while num_tokens_generated <= self.max_tokens:\n",
        "            \n",
        "#             # Pad length \n",
        "#             pad_len = maxlen - len(start_tokens)\n",
        "            \n",
        "#             # The index of the last token in the start_tokens\n",
        "#             sample_index = len(start_tokens) - 1\n",
        "            \n",
        "#             # If the pad length is less than 0\n",
        "#             if pad_len < 0:\n",
        "                \n",
        "#                 # Inputs: Start tokens from 0 to maxlen\n",
        "#                 x = start_tokens[:maxlen]\n",
        "                \n",
        "#                 # Set the sample index to maxlen - 1\n",
        "#                 sample_index = maxlen - 1\n",
        "                \n",
        "#             # If the pad length is greater than 0\n",
        "#             elif pad_len > 0:\n",
        "                \n",
        "#                 # Inputs: Start tokens and pad with 0s\n",
        "#                 x = start_tokens + [0] * pad_len\n",
        "                \n",
        "#             # If the pad length is 0\n",
        "#             else:\n",
        "                \n",
        "#                 # Inputs: Start tokens\n",
        "#                 x = start_tokens\n",
        "                \n",
        "#             # Convert to numpy array\n",
        "#             x = np.array([x])\n",
        "            \n",
        "#             # Predict\n",
        "#             y, _ = self.model.predict(x)\n",
        "            \n",
        "#             # Sample from the model\n",
        "#             sample_token = self.sample_from(y[0][sample_index])\n",
        "            \n",
        "#             # Append the sample token to the generated tokens\n",
        "#             tokens_generated.append(sample_token)\n",
        "            \n",
        "#             # Append the sample token to the start tokens\n",
        "#             start_tokens.append(sample_token)\n",
        "            \n",
        "#             # Increment the number of tokens generated\n",
        "#             num_tokens_generated = len(tokens_generated)\n",
        "            \n",
        "#         # Join the predicted tokens \n",
        "#         txt = \" \".join([self.detokenize(_) for _ in self.start_tokens + tokens_generated])\n",
        "        \n",
        "#         # Report\n",
        "#         print(f\"GENERATED TEXT:\\n{txt}\\n\")\n",
        "\n",
        "# # Initialize a word2index dictionary\n",
        "# word_to_index = {}\n",
        "\n",
        "# # Loop over the vocabulary\n",
        "# for index, word in enumerate(vocab):\n",
        "    \n",
        "#     # Add word and index to the dictionary\n",
        "#     word_to_index[word] = index\n",
        "\n",
        "# # Start prompt\n",
        "# start_prompt = \"the wizard \"\n",
        "\n",
        "# # Convert the start prompt to token indices\n",
        "# start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n",
        "\n",
        "# # Number of tokens to be generated\n",
        "# num_tokens_generated = 50\n",
        "\n",
        "# # Initialize the text generator\n",
        "# text_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>\n",
        "\n",
        "### EVALUATION\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "text_generation_with_miniature_gpt",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "c992f0b96e0bf1b7bf4ed1da6ffb7bb064b2bb3e7201b712e1347652797d5077"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
